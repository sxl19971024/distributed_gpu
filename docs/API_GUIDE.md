# API ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜æ¡†æ¶ä¸­æ¯ä¸ªæ¨¡å—å’Œåˆ†å¸ƒå¼ç®—æ³•çš„ä½¿ç”¨æ–¹æ³•ã€å‚æ•°å’Œç¤ºä¾‹ã€‚

## ğŸ“‹ ç›®å½•

1. [åˆå§‹åŒ–æ¡†æ¶](#åˆå§‹åŒ–æ¡†æ¶)
2. [çŸ©é˜µè¿ç®—](#çŸ©é˜µè¿ç®—)
3. [åˆ›æ–°ç®—å­ï¼šæ··åˆç²¾åº¦ä¸ç¨€ç–æ„ŸçŸ¥](#åˆ›æ–°ç®—å­æ··åˆç²¾åº¦ä¸ç¨€ç–æ„ŸçŸ¥)
4. [å·ç§¯æ“ä½œ](#å·ç§¯æ“ä½œ)
5. [å‚…é‡Œå¶å˜æ¢](#å‚…é‡Œå¶å˜æ¢)
6. [åˆ›æ–°ç®—å­ï¼šPencil åˆ†è§£ 2D FFT](#åˆ›æ–°ç®—å­pencil-åˆ†è§£-2d-fft)
7. [Einsteinæ±‚å’Œ](#einsteinæ±‚å’Œ)
8. [å½’çº¦æ“ä½œ](#å½’çº¦æ“ä½œ)
9. [åˆ›æ–°ç®—å­ï¼šKahan è¡¥å¿æ±‚å’Œ](#åˆ›æ–°ç®—å­kahan-è¡¥å¿æ±‚å’Œ)
10. [åˆ›æ–°ç®—å­ï¼šStencil è®¡ç®—ä¸ Jacobi è¿­ä»£](#åˆ›æ–°ç®—å­stencil-è®¡ç®—ä¸-jacobi-è¿­ä»£)
11. [ä»£ä»·æ¨¡å‹ä¸è‡ªé€‚åº”ç­–ç•¥](#ä»£ä»·æ¨¡å‹ä¸è‡ªé€‚åº”ç­–ç•¥)
12. [æµæ°´çº¿ä¼˜åŒ–å™¨](#æµæ°´çº¿ä¼˜åŒ–å™¨)
13. [é”™è¯¯å¤„ç†ä¸å®¹é”™](#é”™è¯¯å¤„ç†ä¸å®¹é”™)
14. [GPU ç®¡ç†ä¸æ€§èƒ½åˆ†æ](#gpu-ç®¡ç†ä¸æ€§èƒ½åˆ†æ)

---

## åˆå§‹åŒ–æ¡†æ¶

åœ¨ä½¿ç”¨ä»»ä½•åˆ†å¸ƒå¼ç®—æ³•ä¹‹å‰ï¼Œå¿…é¡»å…ˆåˆå§‹åŒ– MPI ç¯å¢ƒå’Œå¼ é‡åˆ†é…å™¨ã€‚

```python
import torch
from distributed_gpu.mpi_manager import MPIManager
from distributed_gpu.tensor_distributor import TensorDistributor

# åˆå§‹åŒ–ï¼ˆæ‰€æœ‰è¿›ç¨‹éƒ½å¿…é¡»æ‰§è¡Œï¼‰
mpi = MPIManager()
distributor = TensorDistributor(mpi)

# è·å–å½“å‰è¿›ç¨‹ä¿¡æ¯
print(f"è¿›ç¨‹ {mpi.get_rank()}/{mpi.get_size()}, GPU {mpi.get_gpu_id()}")
```

**é‡è¦åŸåˆ™**ï¼š
- âš ï¸ æ‰€æœ‰è¿›ç¨‹éƒ½å¿…é¡»è°ƒç”¨ç›¸åŒçš„åˆ†å¸ƒå¼å‡½æ•°ï¼ˆå¦åˆ™ MPI ä¼šæ­»é”ï¼‰
- âš ï¸ è¾“å…¥æ•°æ®ä»…åœ¨ä¸»è¿›ç¨‹ï¼ˆrank=0ï¼‰æä¾›ï¼Œå…¶ä»–è¿›ç¨‹ä¼  `None`
- âš ï¸ è¾“å‡ºç»“æœä»…ä¸»è¿›ç¨‹è¿”å›æœ‰æ•ˆå€¼ï¼Œå…¶ä»–è¿›ç¨‹è¿”å› `None`

---

## çŸ©é˜µè¿ç®—

### 1. distributed_matmul â€” çŸ©é˜µä¹˜æ³•

**åŠŸèƒ½**ï¼šè®¡ç®— C = A @ Bï¼Œæ”¯æŒä¸‰ç§åˆ†å‰²ç­–ç•¥ï¼Œå¯é€šè¿‡ä»£ä»·æ¨¡å‹è‡ªåŠ¨é€‰æ‹©ã€‚

```python
from distributed_gpu.algorithms.matrix_ops import distributed_matmul
from distributed_gpu.cost_model import CostModel, ClusterConfig, SplitStrategy

C = distributed_matmul(
    A,                    # torch.Tensor [M, K]ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    B,                    # torch.Tensor [K, N]ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    mpi,                  # MPIManager
    distributor,          # TensorDistributor
    cost_model=None,      # CostModel: ä¼ å…¥åˆ™è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥
    strategy=None         # SplitStrategy: å¼ºåˆ¶æŒ‡å®šç­–ç•¥ï¼ˆä¼˜å…ˆçº§é«˜äº cost_modelï¼‰
)
# è¿”å›: torch.Tensor [M, N]ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
```

**strategy å‚æ•°ï¼ˆå¯é€‰ï¼‰**ï¼š

| å€¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|---|---|---|
| `SplitStrategy.ROW_SPLIT` | æŒ‰ A çš„è¡Œåˆ†å‰² | M â‰« N |
| `SplitStrategy.COLUMN_SPLIT` | æŒ‰ B çš„åˆ—åˆ†å‰² | N â‰« M |
| `SplitStrategy.BLOCK_2D` | 2D å—åˆ†å‰²ï¼ˆSUMMA é£æ ¼ï¼‰ | M â‰ˆ Nï¼Œé™ä½æ¯å¡æ˜¾å­˜ |

**ç¤ºä¾‹ â€” é»˜è®¤è¡Œåˆ†å‰²**ï¼š

```python
if mpi.is_master_process():
    A = torch.randn(5000, 3000).cuda()
    B = torch.randn(3000, 4000).cuda()
else:
    A, B = None, None

C = distributed_matmul(A, B, mpi, distributor)
```

**ç¤ºä¾‹ â€” ä»£ä»·æ¨¡å‹è‡ªåŠ¨é€‰æ‹©**ï¼š

```python
config = ClusterConfig.from_auto_detect(num_nodes=mpi.get_size())
cost_model = CostModel(config)

C = distributed_matmul(A, B, mpi, distributor, cost_model=cost_model)
# æ¡†æ¶ä¼šè‡ªåŠ¨åœ¨è¡Œåˆ†å‰²/åˆ—åˆ†å‰²/2Då—åˆ†å‰²ä¸­é€‰æ‹©é¢„ä¼°è€—æ—¶æœ€çŸ­çš„ç­–ç•¥
```

---

### 2. distributed_batch_matmul â€” æ‰¹é‡çŸ©é˜µä¹˜æ³•

```python
from distributed_gpu.algorithms.matrix_ops import distributed_batch_matmul

C = distributed_batch_matmul(
    A,              # torch.Tensor [batch, M, K]
    B,              # torch.Tensor [batch, K, N] æˆ– [K, N]
    mpi,
    distributor
)
# è¿”å›: torch.Tensor [batch, M, N]
```

---

### 3. distributed_transpose â€” çŸ©é˜µè½¬ç½®

```python
from distributed_gpu.algorithms.matrix_ops import distributed_transpose

A_T = distributed_transpose(A, mpi, distributor, dim0=0, dim1=1)
```

---

### 4. distributed_add â€” å¼ é‡åŠ æ³•

```python
from distributed_gpu.algorithms.matrix_ops import distributed_add

C = distributed_add(A, B, mpi, distributor)  # C = A + B
```

---

## åˆ›æ–°ç®—å­ï¼šæ··åˆç²¾åº¦ä¸ç¨€ç–æ„ŸçŸ¥

### 5. distributed_matmul_mixed_precision â€” æ··åˆç²¾åº¦çŸ©é˜µä¹˜æ³• â­

**åŠŸèƒ½**ï¼šé€šä¿¡æ—¶ä½¿ç”¨ FP16ï¼Œè®¡ç®—æ—¶ä½¿ç”¨ FP32ã€‚é€šä¿¡é‡å‡å°‘çº¦ 50%ã€‚

**è¯¯å·®ä¸Šç•Œ**ï¼šâ€–C\_mixed - C\_exactâ€– â‰¤ O(Îµ\_FP16 Ã— âˆšK Ã— â€–Aâ€– Ã— â€–Bâ€–)

```python
from distributed_gpu.algorithms.matrix_ops import distributed_matmul_mixed_precision

C = distributed_matmul_mixed_precision(
    A,                    # torch.Tensor [M, K]ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    B,                    # torch.Tensor [K, N]ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    mpi,
    distributor,
    comm_dtype=torch.float16  # é€šä¿¡ç²¾åº¦ï¼ˆå¯é€‰ torch.bfloat16ï¼‰
)
```

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤§çŸ©é˜µä¹˜æ³•ä¸­é€šä¿¡æˆä¸ºç“¶é¢ˆæ—¶
- ç»“æœç²¾åº¦è¦æ±‚åœ¨ 1e-3 é‡çº§å³å¯çš„åœºæ™¯
- è¿­ä»£ç®—æ³•ä¸­é—´æ­¥éª¤ï¼ˆæœ€ç»ˆç»“æœåœ¨å…¨ç²¾åº¦ä¸‹æ”¶æ•›ï¼‰

---

### 6. distributed_matmul_sparse_aware â€” ç¨€ç–æ„ŸçŸ¥çŸ©é˜µä¹˜æ³• â­

**åŠŸèƒ½**ï¼šè‡ªåŠ¨æ£€æµ‹çŸ©é˜µç¨€ç–åº¦ï¼Œå†³å®šä½¿ç”¨ç¨ å¯†æˆ–ç¨€ç–ï¼ˆCOO æ ¼å¼å¹¿æ’­ï¼‰è·¯å¾„ã€‚

```python
from distributed_gpu.algorithms.matrix_ops import distributed_matmul_sparse_aware

C = distributed_matmul_sparse_aware(
    A,                        # torch.Tensor [M, K]ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    B,                        # torch.Tensor [K, N]ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    mpi,
    distributor,
    sparsity_threshold=0.5    # ç¨€ç–åº¦é˜ˆå€¼ï¼ˆè¶…è¿‡åˆ™ä½¿ç”¨COOå¹¿æ’­ï¼‰
)
```

**é€šä¿¡é‡å¯¹æ¯”**ï¼š
- ç¨ å¯†å¹¿æ’­ Bï¼šK Ã— N Ã— 4 å­—èŠ‚
- COO å¹¿æ’­ Bï¼šnnz Ã— 20 å­—èŠ‚ï¼ˆç¨€ç–åº¦ 80% æ—¶é€šä¿¡é‡å‡å°‘ 60%ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šæœ‰é™å…ƒåˆšåº¦çŸ©é˜µã€å›¾é‚»æ¥çŸ©é˜µã€ç¨€ç–ç‰©ç†ç›¸äº’ä½œç”¨çŸ©é˜µã€‚

---

## å·ç§¯æ“ä½œ

### distributed_conv2d â€” 2D å·ç§¯

```python
from distributed_gpu.algorithms.convolution import distributed_conv2d

output = distributed_conv2d(
    input,              # torch.Tensor [N, C_in, H, W]
    weight,             # torch.Tensor [C_out, C_in, kH, kW]
    mpi,
    distributor,
    bias=None,          # torch.Tensor [C_out]ï¼ˆå¯é€‰ï¼‰
    stride=(1, 1),
    padding=(0, 0),
    dilation=(1, 1),
    groups=1
)
```

---

## å‚…é‡Œå¶å˜æ¢

### 1. distributed_fft â€” 1D FFT

```python
from distributed_gpu.algorithms.fft import distributed_fft

output = distributed_fft(input, mpi, distributor, n=None, dim=-1, norm="backward")
```

### 2. distributed_ifft â€” é€† FFT

```python
from distributed_gpu.algorithms.fft import distributed_ifft

output = distributed_ifft(input, mpi, distributor, n=None, dim=-1, norm="backward")
```

### 3. distributed_fft2d â€” 2D FFTï¼ˆæŒ‰ batch åˆ†å‰²ï¼‰

```python
from distributed_gpu.algorithms.fft import distributed_fft2d

output = distributed_fft2d(input, mpi, distributor, s=None, dim=(-2, -1), norm="backward")
```

**æ³¨æ„**ï¼šæ­¤å‡½æ•°æŒ‰ batch ç»´åº¦ï¼ˆdim=0ï¼‰åˆ†å‰²ï¼Œè¦æ±‚è¾“å…¥è‡³å°‘ä¸º 3D `[batch, H, W]`ã€‚é€‚ç”¨äºå¤šä¸ªå°/ä¸­ç½‘æ ¼çš„å¹¶è¡Œ FFTã€‚å¯¹äºå•å¼ è¶…å¤§ 2D ç½‘æ ¼ï¼Œè¯·ä½¿ç”¨ `distributed_fft2d_pencil`ã€‚

### 4. distributed_rfft â€” å®æ•° FFT

```python
from distributed_gpu.algorithms.fft import distributed_rfft

output = distributed_rfft(input, mpi, distributor, n=None, dim=-1, norm="backward")
# è¾“å‡ºé•¿åº¦: N/2 + 1ï¼ˆåˆ©ç”¨å…±è½­å¯¹ç§°æ€§ï¼Œè®¡ç®—é‡å’Œé€šä¿¡é‡çº¦å‡åŠï¼‰
```

**é€‚ç”¨åœºæ™¯**ï¼šç§‘å­¦è®¡ç®—ä¸­ç»å¤§å¤šæ•°ç‰©ç†é‡ï¼ˆæ¸©åº¦ã€å‹åŠ›ã€é€Ÿåº¦ã€ç”µç£åœºï¼‰éƒ½æ˜¯å®æ•°ã€‚

---

## åˆ›æ–°ç®—å­ï¼šPencil åˆ†è§£ 2D FFT

### distributed_fft2d_pencil â­

**åŠŸèƒ½**ï¼šåŸºäº Pencil åˆ†è§£çš„åˆ†å¸ƒå¼ 2D FFTã€‚æ²¿å˜æ¢ç»´åº¦åˆ†å‰²ï¼ˆè€Œé batch ç»´åº¦ï¼‰ï¼Œèƒ½å¤„ç†**å•å¼ è¶…å¤§ 2D ç½‘æ ¼**ã€‚

```python
from distributed_gpu.algorithms.fft import distributed_fft2d_pencil

output = distributed_fft2d_pencil(
    input,              # torch.Tensor [H, W]ï¼ˆä»…ä¸»è¿›ç¨‹ï¼Œ2Dï¼‰
    mpi,
    distributor,
    norm="backward"
)
```

**ç®—æ³•æ­¥éª¤**ï¼š
1. æŒ‰è¡Œåˆ†å‘ â†’ å„è¿›ç¨‹æŒæœ‰ `[local_H, W]`
2. æ²¿ W æ–¹å‘ 1D FFT
3. **All-to-All è½¬ç½®** â†’ å„è¿›ç¨‹æŒæœ‰ `[H, local_W]`
4. æ²¿ H æ–¹å‘ 1D FFT
5. **All-to-All è½¬ç½®å›** â†’ `[local_H, W]`
6. æ”¶é›†ç»“æœ

**è¦æ±‚**ï¼šH å’Œ W éƒ½èƒ½è¢«è¿›ç¨‹æ•° P æ•´é™¤ã€‚

**ä¸ distributed_fft2d çš„åŒºåˆ«**ï¼š

| | `distributed_fft2d` | `distributed_fft2d_pencil` |
|---|---|---|
| è¾“å…¥ | `[batch, H, W]` | `[H, W]` |
| åˆ†å‰²æ–¹å¼ | batch ç»´ | ç©ºé—´ç»´ï¼ˆH â†’ Wï¼‰ |
| é€šä¿¡æ¨¡å¼ | scatter + gather | **All-to-All** |
| é€‚ç”¨åœºæ™¯ | å¤šä¸ªå°ç½‘æ ¼ | å•ä¸ªè¶…å¤§ç½‘æ ¼ |

---

## Einstein æ±‚å’Œ

### 1. distributed_einsum

```python
from distributed_gpu.algorithms.einsum import distributed_einsum

result = distributed_einsum(
    equation,               # str: 'bij,bjk->bik'
    *operands,              # torch.Tensor
    mpi=mpi,
    distributor=distributor,
    optimize='auto',
    use_opt_einsum=True
)
```

### 2. distributed_einsum_with_path â€” å¸¦é¢„è®¡ç®—è·¯å¾„

```python
from distributed_gpu.algorithms.einsum import distributed_einsum_with_path, get_optimal_path

path, info = get_optimal_path('ij,jk,kl->il', (100,50), (50,80), (80,40))
result = distributed_einsum_with_path('ij,jk,kl->il', A, B, C,
                                       mpi=mpi, distributor=distributor, path=path)
```

### 3. distributed_tensordot

```python
from distributed_gpu.algorithms.einsum import distributed_tensordot

result = distributed_tensordot(a, b, dims=2, mpi=mpi, distributor=distributor)
```

---

## å½’çº¦æ“ä½œ

### distributed_sum / distributed_mean / distributed_max / distributed_min

```python
from distributed_gpu.algorithms.reduction import distributed_sum, distributed_mean
from distributed_gpu.algorithms.reduction import distributed_max, distributed_min

total = distributed_sum(tensor, mpi, distributor, dim=None)
avg = distributed_mean(tensor, mpi, distributor, dim=0, keepdim=True)
maximum = distributed_max(tensor, mpi, distributor)
minimum = distributed_min(tensor, mpi, distributor)
```

**å‚æ•°**ï¼š
- `dim=None`ï¼šå…¨å±€å½’çº¦ï¼ˆè¿”å›æ ‡é‡ï¼‰
- `dim=0`ï¼šæ²¿åˆ†å‰²ç»´åº¦å½’çº¦ï¼ˆéœ€è¦ allreduceï¼‰
- `dim=å…¶ä»–`ï¼šæœ¬åœ°å½’çº¦å gather

---

## åˆ›æ–°ç®—å­ï¼šKahan è¡¥å¿æ±‚å’Œ

### distributed_sum_kahan / distributed_mean_kahan â­

**åŠŸèƒ½**ï¼šä½¿ç”¨ float64 ä¸­é—´ç²¾åº¦ + Kahan è¡¥å¿ç®—æ³•ï¼Œè¯¯å·®ä» O(nÂ·Îµ) é™ä½åˆ° O(Îµ)ã€‚

```python
from distributed_gpu.algorithms.reduction import distributed_sum_kahan, distributed_mean_kahan

# æ•°å€¼ç¨³å®šçš„å…¨å±€æ±‚å’Œ
total = distributed_sum_kahan(tensor, mpi, distributor, dim=None)

# æ•°å€¼ç¨³å®šçš„å…¨å±€å‡å€¼
avg = distributed_mean_kahan(tensor, mpi, distributor, dim=None)
```

**ç²¾åº¦å¯¹æ¯”**ï¼ˆå®æµ‹ï¼‰ï¼š

| æ–¹æ³• | è¯¯å·®é‡çº§ | æ€§èƒ½ |
|---|---|---|
| `distributed_sum` | ~1e-5 | å¿« |
| `distributed_sum_kahan` | ~1e-7 | è¾ƒæ…¢ï¼ˆfloat64 + å—è¡¥å¿ï¼‰ |

**é€‚ç”¨åœºæ™¯**ï¼š
- èƒ½é‡å®ˆæ’éªŒè¯ï¼ˆè¯¯å·®ç´¯ç§¯ â†’ ç‰©ç†ç»“æœå¤±çœŸï¼‰
- é•¿æ—¶é—´æ­¥ç§¯åˆ†
- å¤šæ¬¡è¿­ä»£ä¸­é—´å€¼ç´¯åŠ 

---

## åˆ›æ–°ç®—å­ï¼šStencil è®¡ç®—ä¸ Jacobi è¿­ä»£

### distributed_stencil_2d â­

**åŠŸèƒ½**ï¼šåˆ†å¸ƒå¼ 2D Stencil è®¡ç®—ï¼Œé€šè¿‡ Halo Exchange äº¤æ¢è¾¹ç•Œæ•°æ®ã€‚

```python
from distributed_gpu.algorithms.stencil import distributed_stencil_2d

result = distributed_stencil_2d(
    grid,                   # torch.Tensor [H, W]ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    mpi,
    distributor,
    stencil_kernel=None,    # é»˜è®¤: 5ç‚¹Laplacian [[0,1,0],[1,-4,1],[0,1,0]]
    boundary='zero',        # 'zero' æˆ– 'periodic'
    iterations=1            # è¿­ä»£æ¬¡æ•°
)
```

**Halo Exchange å®ç°**ï¼š
- ä½¿ç”¨ MPI `Sendrecv`ï¼ˆå¤§å†™/ç¼“å†²åŒºç‰ˆæœ¬ï¼‰+ `MPI.PROC_NULL` è¾¹ç•Œå¤„ç†
- **æ— æ­»é”ä¿è¯**ï¼šæ‰€æœ‰è¿›ç¨‹åŒæ­¥å‚ä¸é€šä¿¡
- é€šä¿¡é‡ï¼šæ¯æ¬¡è¿­ä»£æ¯è¿›ç¨‹ä»… O(W) å­—èŠ‚ï¼ˆè¾¹ç•Œè¡Œï¼‰

**é¢„å®šä¹‰ Stencil æ ¸**ï¼š

```python
from distributed_gpu.algorithms.stencil import DEFAULT_LAPLACIAN_5PT, DEFAULT_LAPLACIAN_9PT

# 5ç‚¹: [[0,1,0],[1,-4,1],[0,1,0]]    â€” äºŒé˜¶ç²¾åº¦
# 9ç‚¹: [[1,4,1],[4,-20,4],[1,4,1]]/6  â€” æ›´é«˜ç²¾åº¦
```

---

### distributed_jacobi_2d â­

**åŠŸèƒ½**ï¼šåˆ†å¸ƒå¼ Jacobi è¿­ä»£æ±‚è§£ 2D æ³Šæ¾æ–¹ç¨‹ âˆ‡Â²u = fã€‚

```python
from distributed_gpu.algorithms.stencil import distributed_jacobi_2d

solution = distributed_jacobi_2d(
    grid,                   # åˆå§‹çŒœæµ‹ [H, W]ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    rhs,                    # å³ç«¯é¡¹ f [H, W]ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    mpi,
    distributor,
    dx=1.0,                 # ç½‘æ ¼é—´è·
    boundary='zero',
    iterations=1000,        # æœ€å¤§è¿­ä»£æ¬¡æ•°
    tol=1e-6                # æ”¶æ•›å®¹å·®
)
```

**ç‰¹æ€§**ï¼šè‡ªåŠ¨æ”¶æ•›æ£€æµ‹ï¼ˆå…¨å±€æ®‹å·® < tol æ—¶æå‰ç»ˆæ­¢ï¼‰ã€‚

---

## ä»£ä»·æ¨¡å‹ä¸è‡ªé€‚åº”ç­–ç•¥

### ClusterConfig

```python
from distributed_gpu.cost_model import ClusterConfig

config = ClusterConfig.from_auto_detect(num_nodes=4)  # è‡ªåŠ¨æ£€æµ‹
```

### CostModel

```python
from distributed_gpu.cost_model import CostModel, SplitStrategy

cost_model = CostModel(config)

# ä¼°ç®—ç‰¹å®šç­–ç•¥çš„ä»£ä»·
cost = cost_model.estimate_matmul_cost(M=5000, K=5000, N=5000,
                                        strategy=SplitStrategy.ROW_SPLIT)

# è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥
plan = cost_model.find_optimal_strategy(M=5000, K=5000, N=5000)
print(f"æ¨è: {plan.strategy.value}")

# æ‰“å°å®Œæ•´åˆ†æ
cost_model.print_analysis(M=5000, K=5000, N=5000)
```

---

## æµæ°´çº¿ä¼˜åŒ–å™¨

```python
from distributed_gpu.pipeline_optimizer import PipelineOptimizer, PipelineConfig

pipeline = PipelineOptimizer(mpi, PipelineConfig(num_chunks=4))

# æµæ°´çº¿çŸ©é˜µä¹˜æ³•
C = pipeline.pipelined_matmul(A, B, num_chunks=4)

# æµæ°´çº¿ AllReduce
result = pipeline.pipelined_allreduce(tensor, num_chunks=4)

# ç†è®ºæ”¶ç›Šä¼°ç®—
benefit = pipeline.estimate_overlap_benefit(
    compute_time_ms=10.0, comm_time_ms=5.0, num_chunks=4
)
```

---

## é”™è¯¯å¤„ç†ä¸å®¹é”™

### MPIError

```python
from distributed_gpu.mpi_manager import MPIError

try:
    C = distributed_matmul(A, B, mpi, distributor)
except MPIError as e:
    print(f"MPI é”™è¯¯ (rank {e.rank}): {e}")
```

### check_health

```python
alive = mpi.check_health()  # è½»é‡çº§å¿ƒè·³æ£€æµ‹
```

---

## GPU ç®¡ç†ä¸æ€§èƒ½åˆ†æ

### GPUManager

```python
from distributed_gpu.gpu_manager import GPUManager

gpu = GPUManager(mpi.get_gpu_id())
gpu.print_info()
gpu.print_memory_info()
```

### Profiler

```python
from distributed_gpu.utils.profiler import Profiler

profiler = Profiler(enabled=mpi.is_master_process())
profiler.start("matmul")
C = distributed_matmul(A, B, mpi, distributor)
profiler.end("matmul")
profiler.print_summary()
```

### é¢„çƒ­ï¼ˆWarmupï¼‰

```python
# GPU / MPI é¢„çƒ­ï¼šæ¶ˆé™¤é¦–æ¬¡è°ƒç”¨çš„ CUDA å†…æ ¸ç¼–è¯‘å’Œ MPI å»ºé“¾å»¶è¿Ÿ
from distributed_gpu.algorithms.matrix_ops import distributed_matmul
for _ in range(3):
    if mpi.is_master_process():
        t = torch.randn(64, 64).cuda()
    else:
        t = None
    distributed_matmul(t, t, mpi, distributor)
mpi.synchronize()
```

---

## å®Œæ•´ç¤ºä¾‹

```python
#!/usr/bin/env python
"""è¿è¡Œ: mpirun -n 4 python example.py"""
import torch
from distributed_gpu.mpi_manager import MPIManager
from distributed_gpu.tensor_distributor import TensorDistributor
from distributed_gpu.cost_model import CostModel, ClusterConfig
from distributed_gpu.algorithms.matrix_ops import (distributed_matmul,
                                        distributed_matmul_mixed_precision,
                                        distributed_matmul_sparse_aware)
from distributed_gpu.algorithms.fft import distributed_fft2d_pencil
from distributed_gpu.algorithms.reduction import distributed_sum_kahan
from distributed_gpu.algorithms.stencil import distributed_stencil_2d, distributed_jacobi_2d

def main():
    mpi = MPIManager()
    distributor = TensorDistributor(mpi)

    # === çŸ©é˜µä¹˜æ³•ï¼ˆä»£ä»·æ¨¡å‹è‡ªåŠ¨é€‰æ‹©ç­–ç•¥ï¼‰===
    config = ClusterConfig.from_auto_detect(mpi.get_size())
    cost_model = CostModel(config)
    
    if mpi.is_master_process():
        A = torch.randn(2000, 1500).cuda()
        B = torch.randn(1500, 1000).cuda()
    else:
        A, B = None, None
    
    C = distributed_matmul(A, B, mpi, distributor, cost_model=cost_model)
    
    # === æ··åˆç²¾åº¦çŸ©é˜µä¹˜æ³•ï¼ˆé€šä¿¡é‡å‡åŠï¼‰===
    C_mp = distributed_matmul_mixed_precision(A, B, mpi, distributor)
    
    # === Pencil 2D FFTï¼ˆè¶…å¤§å•ç½‘æ ¼ï¼‰===
    if mpi.is_master_process():
        field = torch.randn(1024, 1024).cuda()
    else:
        field = None
    spectrum = distributed_fft2d_pencil(field, mpi, distributor)
    
    # === Kahan è¡¥å¿æ±‚å’Œï¼ˆé«˜ç²¾åº¦ï¼‰===
    if mpi.is_master_process():
        data = torch.randn(1000, 1000, 100).cuda()
    else:
        data = None
    total = distributed_sum_kahan(data, mpi, distributor)
    
    # === Stencil è®¡ç®—ï¼ˆç‰©ç†æ¨¡æ‹Ÿï¼‰===
    if mpi.is_master_process():
        grid = torch.zeros(256, 256).cuda()
        grid[128, 128] = 1000.0  # çƒ­æº
        rhs = torch.zeros(256, 256).cuda()
    else:
        grid, rhs = None, None
    
    result = distributed_stencil_2d(grid, mpi, distributor, iterations=50)
    
    # === Jacobi è¿­ä»£ï¼ˆæ³Šæ¾æ–¹ç¨‹æ±‚è§£ï¼‰===
    solution = distributed_jacobi_2d(grid, rhs, mpi, distributor,
                                      dx=0.01, iterations=500, tol=1e-5)
    
    if mpi.is_master_process():
        print("æ‰€æœ‰æ“ä½œå®Œæˆï¼")

if __name__ == "__main__":
    main()
```

---

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆå…¶ä»–è¿›ç¨‹è¦ä¼  Noneï¼Ÿ
æ•°æ®åªåœ¨ä¸»è¿›ç¨‹å­˜åœ¨ï¼Œåˆ†å¸ƒå¼å‡½æ•°å†…éƒ¨è‡ªåŠ¨å°†æ•°æ®åˆ†å‘åˆ°å…¶ä»–è¿›ç¨‹ã€‚

### Q2: ä¸ºä»€ä¹ˆæ‰€æœ‰è¿›ç¨‹éƒ½è¦è°ƒç”¨å‡½æ•°ï¼Ÿ
MPI é›†åˆé€šä¿¡è¦æ±‚æ‰€æœ‰è¿›ç¨‹åŒæ­¥å‚ä¸ï¼Œå¦åˆ™ä¼šæ­»é”ã€‚

### Q3: distributed_sum å’Œ distributed_sum_kahan æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
å‰è€…é€Ÿåº¦å¿«ï¼ˆ~0.01sï¼‰ï¼Œåè€…ç²¾åº¦é«˜ï¼ˆè¯¯å·® O(Îµ) vs O(nÂ·Îµ)ï¼Œçº¦æ…¢ 10-15 å€ï¼‰ã€‚æ™®é€šåœºæ™¯ç”¨ sumï¼Œèƒ½é‡å®ˆæ’éªŒè¯ç­‰é«˜ç²¾åº¦éœ€æ±‚ç”¨ sum_kahanã€‚

### Q4: distributed_fft2d å’Œ distributed_fft2d_pencil æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
å‰è€…æŒ‰ batch åˆ†å‰²ï¼ˆé€‚åˆå¤šä¸ªå°ç½‘æ ¼ï¼‰ï¼Œåè€…æŒ‰ç©ºé—´ç»´åº¦åˆ†å‰²ï¼ˆé€‚åˆå•å¼ è¶…å¤§ç½‘æ ¼ï¼‰ã€‚å•ä¸ª 2D ç½‘æ ¼åªèƒ½ç”¨ pencil ç‰ˆæœ¬ã€‚

### Q5: å¦‚ä½•è°ƒæ•´ GPU æ•°é‡ï¼Ÿ
ä¿®æ”¹ `mpirun -n <æ•°é‡>` å‚æ•°å³å¯ã€‚2D å—åˆ†å‰²è¦æ±‚è¿›ç¨‹æ•°å¯åˆ†è§£ä¸º â‰¥2Ã—2 çš„ç½‘æ ¼ã€‚Pencil FFT è¦æ±‚ H å’Œ W éƒ½èƒ½è¢«è¿›ç¨‹æ•°æ•´é™¤ã€‚

---

## AutoExecutor â€” æ˜¾å­˜æ„ŸçŸ¥è‡ªåŠ¨åŒ–åˆ†å¸ƒå¼è®¡ç®—

### æ¦‚è¿°

`AutoExecutor` æ˜¯æ¡†æ¶çš„é«˜å±‚ç”¨æˆ·æ¥å£ï¼Œå®ç°äº†**æ˜¾å­˜æ„ŸçŸ¥çš„è‡ªé€‚åº”èµ„æºè°ƒåº¦**ã€‚
ç”¨æˆ·åªéœ€æä¾› CPU å¼ é‡ï¼Œæ¡†æ¶è‡ªåŠ¨å®Œæˆï¼š

1. **å®æ—¶æ‰«æ** æ‰€æœ‰ GPU çš„çœŸå®å¯ç”¨æ˜¾å­˜ï¼ˆOS çº§åˆ«ï¼Œå«å…¶ä»–è¿›ç¨‹å ç”¨ï¼‰
2. **æ™ºèƒ½è§„åˆ’** æ ¹æ®æ•°æ®é‡å’Œå¯ç”¨æ˜¾å­˜å†³å®šåˆ†æ‰¹ç­–ç•¥
3. **è‡ªåŠ¨æ‰§è¡Œ** åˆ†å¸ƒå¼è®¡ç®— + åˆ†æ‰¹å¤„ç† + ç»“æœæ‹¼æ¥
4. **è¿”å›ç»“æœ** CPU å¼ é‡ï¼ˆä»… master è¿›ç¨‹ï¼‰

### åˆå§‹åŒ–

```python
from distributed_gpu.auto_executor import AutoExecutor

# æ‰€æœ‰è¿›ç¨‹éƒ½å¿…é¡»è°ƒç”¨
executor = AutoExecutor(
    verbose=True,           # æ˜¯å¦æ‰“å°æ‰§è¡Œä¿¡æ¯
    max_per_gpu_gb=None,    # äººä¸ºé™åˆ¶æ¯GPUå¯ç”¨æ˜¾å­˜ (æµ‹è¯•ç”¨)
)
```

### GPU æ˜¾å­˜çŠ¶æ€æŸ¥çœ‹

```python
executor.gpu_status()
# è¾“å‡º:
# ======================================================================
#   GPU æ˜¾å­˜çŠ¶æ€ï¼ˆå®æ—¶æ£€æµ‹ï¼Œå«å…¶ä»–è¿›ç¨‹å ç”¨ï¼‰
# ----------------------------------------------------------------------
#   GPU 0 (Rank  0): [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 4.1 / 31.4 GB ç©ºé—² | å®‰å…¨å¯ç”¨ 3.3 GB
#   GPU 1 (Rank  1): [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 30.4 / 31.4 GB ç©ºé—² | å®‰å…¨å¯ç”¨ 24.3 GB
# ======================================================================
```

### MatMulï¼ˆæ”¯æŒè¶…æ˜¾å­˜è‡ªåŠ¨åˆ†æ‰¹ï¼‰

```python
if executor.is_master:
    A = torch.randn(100000, 10000)  # CPU å¼ é‡
    B = torch.randn(10000, 5000)
else:
    A = B = None

C = executor.matmul(A, B)  # è‡ªåŠ¨: æ˜¾å­˜æ‰«æâ†’åˆ†æ‰¹è§„åˆ’â†’åˆ†å¸ƒå¼è®¡ç®—â†’æ‹¼æ¥è¿”å›
# C æ˜¯ CPU tensor (ä»… master)
```

**åˆ†æ‰¹ä¼˜åŒ–**ï¼šå½“æ•°æ®è¶…è¿‡ GPU æ€»æ˜¾å­˜æ—¶ï¼ŒA æ²¿è¡Œåˆ†æ‰¹ï¼ŒB åªå¹¿æ’­ä¸€æ¬¡åˆ°æ‰€æœ‰ GPU å¹¶å¤ç”¨ã€‚

### FFT / RFFT / FFT2D

```python
result = executor.fft(signal_cpu)
result = executor.rfft(signal_cpu)
result = executor.fft2d(field_cpu)
```

### Sum / Mean

```python
total = executor.sum(data_cpu)
avg = executor.mean(data_cpu, dim=0)
```

### Conv2d

```python
output = executor.conv2d(input_cpu, weight_cpu, bias=bias_cpu, padding=(1,1))
```

**åˆ†æ‰¹ä¼˜åŒ–**ï¼šweight å’Œ bias åªå¹¿æ’­ä¸€æ¬¡ï¼Œinput æ²¿ batch ç»´åº¦åˆ†æ‰¹ã€‚

### Einsum

```python
C = executor.einsum("ij,jk->ik", A_cpu, B_cpu)
```

### æ‰¹é‡ MatMul

```python
if executor.is_master:
    pairs = [(A1, B1), (A2, B2), (A3, B3)]
else:
    pairs = None

results = executor.matmul_batch(pairs)  # è¿”å› [C1, C2, C3]
```

### æ‰§è¡Œè®¡åˆ’é¢„è§ˆï¼ˆä¸æ‰§è¡Œè®¡ç®—ï¼‰

```python
plan = executor.plan_info("matmul", (50000, 10000), (10000, 50000))
# è¾“å‡º:
# [AutoExecutor] â”â”â” æ‰§è¡Œè®¡åˆ’ â”â”â”
#   æ“ä½œ: matmul
#   ç­–ç•¥: 5 æ‰¹æ¬¡ Ã— 4 GPU å¹¶è¡Œ | æ¯æ‰¹ 23480 è¡Œ | æ¯GPU 3.175 GB
#   æ•°æ®æ€»é‡: 13.039 GB
#   GPUå¯ç”¨æ˜¾å­˜: 75.72 GB (å•å¡æœ€å° 3.17 GB)
```

### ä¾¿æ·å‡½æ•° auto_compute

```python
from distributed_gpu.auto_executor import auto_compute

C = auto_compute("matmul", A_cpu, B_cpu)
Y = auto_compute("fft", X_cpu)
S = auto_compute("sum", data_cpu)
```

### ResourcePlanner API

```python
from distributed_gpu.resource_planner import ResourcePlanner

planner = ResourcePlanner(mpi, max_per_gpu_gb=None)

# æ‰«ææ‰€æœ‰ GPU å¯ç”¨æ˜¾å­˜
statuses = planner.scan_all_gpus()
for s in statuses:
    print(f"GPU {s.gpu_id}: {s.free_memory_gb:.1f}/{s.total_memory_gb:.1f} GB ç©ºé—²")

# ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
plan = planner.plan_matmul(M=50000, K=10000, N=50000)
print(f"å¯è¡Œ: {plan.feasible}, æ‰¹æ¬¡: {plan.num_batches}")
```

---

## å¸¸è§é—®é¢˜ï¼ˆæ›´æ–°ï¼‰

### Q6: AutoExecutor å’Œç›´æ¥è°ƒç”¨ distributed_matmul æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

| ç‰¹æ€§ | `distributed_matmul` | `AutoExecutor.matmul` |
|------|---------------------|----------------------|
| è¾“å…¥ | GPU å¼ é‡ | CPU å¼ é‡ |
| æ˜¾å­˜æ£€æŸ¥ | æ—  | è‡ªåŠ¨æ‰«æå®æ—¶å¯ç”¨æ˜¾å­˜ |
| è¶…æ˜¾å­˜å¤„ç† | OOM å´©æºƒ | è‡ªåŠ¨åˆ†æ‰¹ |
| ç­–ç•¥é€‰æ‹© | éœ€æ‰‹åŠ¨ä¼  cost_model | å†…ç½®è‡ªåŠ¨é€‰æ‹© |
| é€‚ç”¨åœºæ™¯ | å·²çŸ¥æ•°æ®èƒ½æ”¾å…¥ GPU | æ•°æ®é‡ä¸ç¡®å®š / è¶…å¤§è§„æ¨¡ |

### Q7: max_per_gpu_gb æœ‰ä»€ä¹ˆç”¨ï¼Ÿ
ç”¨äºæµ‹è¯•åˆ†æ‰¹é€»è¾‘æˆ–å…±äº« GPU èµ„æºçš„åœºæ™¯ã€‚è®¾ç½®åï¼Œæ¯ GPU å¯ç”¨æ˜¾å­˜ä¸ä¼šè¶…è¿‡è¯¥å€¼ã€‚
ä¾‹å¦‚ `AutoExecutor(max_per_gpu_gb=2.0)` ä¼šè®©æ¯å¼ å¡æœ€å¤šä½¿ç”¨ 2 GBã€‚

### Q8: å®‰å…¨è¾¹é™… 20% å¯ä»¥è°ƒæ•´å—ï¼Ÿ
å¯ä»¥é€šè¿‡ä¿®æ”¹ `ResourcePlanner.SAFETY_MARGIN` å±æ€§ã€‚è¾ƒä½çš„è¾¹é™…æ„å‘³ç€æ›´å……åˆ†åˆ©ç”¨æ˜¾å­˜ä½† OOM é£é™©æ›´é«˜ã€‚
