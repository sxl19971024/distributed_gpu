#!/usr/bin/env python3
"""
è·¨æ¡†æ¶æ€§èƒ½å¯¹æ¯”å®éªŒ (è‡ªé€‚åº”è§„æ¨¡ç‰ˆ)

è‡ªåŠ¨æ£€æµ‹å¯ç”¨GPUå’Œæ˜¾å­˜ï¼Œä»¥2å€é€’å¢çš„è§„æ¨¡è¿è¡Œå®éªŒï¼Œ
è¶…è¿‡å¯ç”¨æ˜¾å­˜æ—¶è‡ªåŠ¨ç»ˆæ­¢å½“å‰å®éªŒè¿›å…¥ä¸‹ä¸€ä¸ªã€‚

è¿è¡Œæ–¹å¼ (ä¸è¦ç”¨ mpirunï¼Œè„šæœ¬å†…éƒ¨ç®¡ç†å¤šè¿›ç¨‹):
  python experiments/benchmark_comparison.py                # è‡ªåŠ¨æ£€æµ‹GPUï¼Œå…¨éƒ¨å®éªŒ
  python experiments/benchmark_comparison.py --gpus 4       # æŒ‡å®šGPUæ•°
  python experiments/benchmark_comparison.py --exp matmul   # åªè·‘æŸä¸ªå®éªŒ
  python experiments/benchmark_comparison.py --list         # æŸ¥çœ‹å¯ç”¨å®éªŒ

å¯¹æ¯”æ¡†æ¶:
  1. PyTorch Distributed (NCCL)  â€” GPUç›´é€šé€šä¿¡åŸºçº¿
  2. PETSc (petsc4py)            â€” ç§‘å­¦è®¡ç®—é‡‘æ ‡å‡†
  3. Dask-CUDA + CuPy            â€” Pythoné€šç”¨åˆ†å¸ƒå¼GPUæ¡†æ¶
  4. æœ¬æ¡†æ¶ (distributed_gpu)    â€” MPI+GPU

æ¯ä¸ªå®éªŒçš„æ•°æ®è§„æ¨¡æŒ‰2å€é€’å¢ (å¦‚ 1024â†’2048â†’4096â†’...)ï¼Œ
ç›´åˆ°é¢„ä¼°æ˜¾å­˜è¶…å‡ºGPUå¯ç”¨æ˜¾å­˜ä¸ºæ­¢ã€‚
"""

from __future__ import annotations

import argparse
import gc
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import torch

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
RESULTS_ROOT = os.path.join(PROJECT_ROOT, "results")

REPEATS = 5       # æ¯ä¸ªè§„æ¨¡é‡å¤æµ‹é‡æ¬¡æ•°
WARMUP = 2        # é¢„çƒ­æ¬¡æ•°
PER_SIZE_TIMEOUT = 120  # å•ä¸ªè§„æ¨¡çš„æœ€å¤§è€—æ—¶ (ç§’)ï¼Œè¶…æ—¶åˆ™è·³è¿‡æ›´å¤§è§„æ¨¡


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  é€šç”¨å·¥å…·
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def detect_free_gpus(min_free_mb: int = 10000) -> List[int]:
    """æ£€æµ‹ç©ºé—²GPU (é€»è¾‘ID)"""
    free = []
    for i in range(torch.cuda.device_count()):
        try:
            f, _ = torch.cuda.mem_get_info(i)
            if f / 1024**2 >= min_free_mb:
                free.append(i)
        except Exception:
            pass
    return free


def get_physical_gpu_ids(logical_ids: List[int]) -> List[int]:
    """é€»è¾‘ID â†’ ç‰©ç†ID (è€ƒè™‘ CUDA_VISIBLE_DEVICES)"""
    cvd = os.environ.get("CUDA_VISIBLE_DEVICES", None)
    if cvd:
        physical = [int(x.strip()) for x in cvd.split(",")]
        return [physical[i] for i in logical_ids if i < len(physical)]
    return logical_ids


def get_min_free_gb(gpu_ids: List[int]) -> float:
    """è·å–æŒ‡å®šGPUä¸­æœ€å°å¯ç”¨æ˜¾å­˜ (GB)"""
    min_free = float("inf")
    for gid in gpu_ids:
        try:
            f, _ = torch.cuda.mem_get_info(gid)
            min_free = min(min_free, f / 1024**3)
        except Exception:
            pass
    return min_free


def make_doubling_sizes(start: int, max_mem_gb: float, mem_per_element_bytes: int,
                        num_elements_fn) -> List[int]:
    """ç”Ÿæˆ2å€é€’å¢çš„è§„æ¨¡åˆ—è¡¨ï¼Œç›´åˆ°è¶…å‡ºæ˜¾å­˜
    Args:
        start: èµ·å§‹è§„æ¨¡
        max_mem_gb: å¯ç”¨æ˜¾å­˜ä¸Šé™ (GB)
        mem_per_element_bytes: æ¯ä¸ªå…ƒç´ å­—èŠ‚æ•° (float32=4)
        num_elements_fn: ç»™å®šè§„æ¨¡Nè¿”å›æ€»å…ƒç´ æ•°çš„å‡½æ•°
    """
    sizes = []
    N = start
    safety = 0.8  # ç•™20%å®‰å…¨è¾¹é™…
    while True:
        mem_gb = num_elements_fn(N) * mem_per_element_bytes / 1024**3
        if mem_gb > max_mem_gb * safety:
            break
        sizes.append(N)
        N *= 2
    return sizes


def benchmark_fn(fn, repeats=REPEATS, warmup=WARMUP, device=None):
    """è®¡æ—¶å·¥å…·"""
    for _ in range(warmup):
        fn()
        if device is not None:
            torch.cuda.synchronize(device)
    times = []
    for _ in range(repeats):
        if device is not None:
            torch.cuda.synchronize(device)
        t0 = time.perf_counter()
        fn()
        if device is not None:
            torch.cuda.synchronize(device)
        t1 = time.perf_counter()
        times.append((t1 - t0) * 1000)
    return float(np.mean(times)), float(np.std(times))


def clean():
    gc.collect()
    torch.cuda.empty_cache()


def save_json(data, filepath):
    with open(filepath, "w") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"  ğŸ’¾ ä¿å­˜: {filepath}")


def make_env(gpu_ids):
    """æ„å»ºå­è¿›ç¨‹ç¯å¢ƒå˜é‡"""
    phys_ids = get_physical_gpu_ids(gpu_ids)
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = ",".join(str(g) for g in phys_ids)
    return env, phys_ids


def run_subprocess(cmd, env, timeout=600, label=""):
    """è¿è¡Œå­è¿›ç¨‹å¹¶æ•è·è¾“å‡º"""
    try:
        proc = subprocess.run(cmd, env=env, capture_output=True, text=True, timeout=timeout)
        if proc.returncode != 0:
            print(f"  âš  {label} å¤±è´¥ (exit={proc.returncode})")
            stderr = proc.stderr[-300:] if proc.stderr else ""
            if stderr:
                print(f"     {stderr}")
            return False
        return True
    except subprocess.TimeoutExpired:
        print(f"  âš  {label} è¶…æ—¶ ({timeout}s)ï¼Œè·³è¿‡")
        return False


def load_tmp_json(filepath):
    """åŠ è½½å¹¶åˆ é™¤ä¸´æ—¶JSON"""
    if os.path.exists(filepath):
        with open(filepath) as f:
            data = json.load(f)
        os.remove(filepath)
        return data
    return []


def remove_if_exists(filepath):
    if os.path.exists(filepath):
        os.remove(filepath)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  å®éªŒ 1: çŸ©é˜µä¹˜æ³• â€” æœ¬æ¡†æ¶ vs NCCL vs å•GPU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def exp_matmul(gpu_ids, output_dir):
    print("\n" + "=" * 70)
    print("  å®éªŒ: çŸ©é˜µä¹˜æ³• â€” æœ¬æ¡†æ¶ vs PyTorch Distributed (NCCL) vs å•GPU")
    print("  è§„æ¨¡: NxN, Nä»1024å¼€å§‹2å€é€’å¢, æ˜¾å­˜ä¸è¶³æ—¶è‡ªåŠ¨åœæ­¢")
    print("=" * 70)

    world_size = len(gpu_ids)
    dev = torch.device(f"cuda:{gpu_ids[0]}")
    free_gb = get_min_free_gb(gpu_ids)
    # matmul: A[N,N] + B[N,N] + C[N,N] = 3*N^2 elements
    sizes = make_doubling_sizes(1024, free_gb, 4, lambda N: 3 * N * N)
    print(f"  GPU: {gpu_ids} ({world_size}å¡) | å¯ç”¨æ˜¾å­˜: {free_gb:.1f}GB")
    print(f"  è§„æ¨¡åºåˆ—: {sizes}")
    if not sizes:
        print("  âš  æ˜¾å­˜ä¸è¶³ï¼Œè·³è¿‡"); return

    # â”€â”€ å•GPU â”€â”€
    print(f"\n  â”€â”€ å•GPUåŸºçº¿ â”€â”€")
    from torch.cuda import empty_cache
    single = []
    for N in sizes:
        A = torch.randn(N, N, device=dev); B = torch.randn(N, N, device=dev)
        m, s = benchmark_fn(lambda: torch.matmul(A, B), device=dev)
        single.append({"size": N, "mean_ms": round(m, 2), "std_ms": round(s, 2)})
        gflops = 2*N**3 / (m/1000) / 1e9
        print(f"    {N:>6}x{N}: {m:>10.2f} Â± {s:.2f} ms  ({gflops:.0f} GFLOPS)")
        del A, B; clean()

    # â”€â”€ NCCL â”€â”€
    print(f"\n  â”€â”€ PyTorch Distributed (NCCL) {world_size}å¡ â”€â”€")
    nccl_file = os.path.join(RESULTS_ROOT, "_tmp_nccl_mm.json")
    nccl_script = os.path.join(RESULTS_ROOT, "_tmp_nccl_mm.py")
    with open(nccl_script, "w") as f:
        f.write(f'''
import os,json,time; import numpy as np; import torch,torch.distributed
rank=int(os.environ["RANK"]); ws=int(os.environ["WORLD_SIZE"])
lr=int(os.environ.get("LOCAL_RANK",rank)); torch.cuda.set_device(lr)
torch.distributed.init_process_group("nccl")
sizes={sizes}; rep={REPEATS}; wu={WARMUP}; results=[]; TLIMIT={PER_SIZE_TIMEOUT}
a=torch.randn(1000,1000,device=f"cuda:{{lr}}")
for _ in range(10): torch.matmul(a,a)
torch.cuda.synchronize(lr); del a; torch.cuda.empty_cache()
for N in sizes:
    sz_t0=time.perf_counter()
    ck=N//ws
    Al=torch.randn(ck,N,device=f"cuda:{{lr}}"); B=torch.randn(N,N,device=f"cuda:{{lr}}")
    for _ in range(wu):
        torch.distributed.broadcast(B,src=0); C=torch.matmul(Al,B)
        cl=[torch.empty_like(C) for _ in range(ws)] if rank==0 else None
        torch.distributed.gather(C,gather_list=cl,dst=0); torch.cuda.synchronize(lr)
    ts=[]
    for _ in range(rep):
        torch.cuda.synchronize(lr); torch.distributed.barrier(); t0=time.perf_counter()
        torch.distributed.broadcast(B,src=0); C=torch.matmul(Al,B)
        cl=[torch.empty_like(C) for _ in range(ws)] if rank==0 else None
        torch.distributed.gather(C,gather_list=cl,dst=0)
        torch.cuda.synchronize(lr); torch.distributed.barrier(); t1=time.perf_counter()
        ts.append((t1-t0)*1000)
    if rank==0: results.append({{"size":N,"mean_ms":round(float(np.mean(ts)),2),"std_ms":round(float(np.std(ts)),2)}})
    del Al,B; torch.cuda.empty_cache()
    if time.perf_counter()-sz_t0>TLIMIT: break
if rank==0:
    with open("{nccl_file}","w") as ff: json.dump(results,ff)
torch.distributed.destroy_process_group()
''')
    env, phys = make_env(gpu_ids)
    cmd = ["torchrun", f"--nproc_per_node={world_size}", "--master_port=29500", nccl_script]
    if run_subprocess(cmd, env, label="NCCL MatMul"):
        nccl = load_tmp_json(nccl_file)
        for r in nccl:
            print(f"    {r['size']:>6}x{r['size']}: {r['mean_ms']:>10.2f} Â± {r['std_ms']:.2f} ms")
    else:
        nccl = []
    remove_if_exists(nccl_script)

    # â”€â”€ æœ¬æ¡†æ¶ â”€â”€
    print(f"\n  â”€â”€ æœ¬æ¡†æ¶ (mpi4py) {world_size}å¡ â”€â”€")
    ours_file = os.path.join(RESULTS_ROOT, "_tmp_ours_mm.json")
    ours_script = os.path.join(RESULTS_ROOT, "_tmp_ours_mm.py")
    with open(ours_script, "w") as f:
        f.write(f'''
import sys,os,json,time,gc; sys.path.insert(0,"{PROJECT_ROOT}")
import numpy as np; import torch
from distributed_gpu.mpi_manager import MPIManager
from distributed_gpu.tensor_distributor import TensorDistributor
from distributed_gpu.algorithms.matrix_ops import distributed_matmul
mpi=MPIManager(); dist=TensorDistributor(mpi); gid=mpi.get_gpu_id()
sizes={sizes}; rep={REPEATS}; wu={WARMUP}; results=[]; TLIMIT={PER_SIZE_TIMEOUT}
a=torch.randn(1000,1000,device=f"cuda:{{gid}}")
for _ in range(10): torch.matmul(a,a)
torch.cuda.synchronize(gid); del a; torch.cuda.empty_cache()
for N in sizes:
    sz_t0=time.perf_counter()
    A=B=None
    if mpi.is_master_process():
        A=torch.randn(N,N,device=f"cuda:{{gid}}"); B=torch.randn(N,N,device=f"cuda:{{gid}}")
    for _ in range(wu):
        mpi.barrier(); distributed_matmul(A,B,mpi,dist); torch.cuda.synchronize(gid); mpi.barrier()
        gc.collect(); torch.cuda.empty_cache()
    ts=[]
    for _ in range(rep):
        mpi.barrier(); torch.cuda.synchronize(gid); t0=time.perf_counter()
        distributed_matmul(A,B,mpi,dist)
        torch.cuda.synchronize(gid); mpi.barrier(); t1=time.perf_counter()
        ts.append((t1-t0)*1000); gc.collect(); torch.cuda.empty_cache()
    if mpi.is_master_process():
        results.append({{"size":N,"mean_ms":round(float(np.mean(ts)),2),"std_ms":round(float(np.std(ts)),2)}})
        del A,B
    gc.collect(); torch.cuda.empty_cache(); mpi.barrier()
    skip=time.perf_counter()-sz_t0>TLIMIT
    skip=mpi.broadcast(skip if mpi.is_master_process() else None)
    if skip: break
if mpi.is_master_process():
    with open("{ours_file}","w") as ff: json.dump(results,ff)
''')
    cmd = ["mpirun", "-n", str(world_size), "--allow-run-as-root", "--oversubscribe",
           sys.executable, ours_script]
    if run_subprocess(cmd, env, label="æœ¬æ¡†æ¶ MatMul"):
        ours = load_tmp_json(ours_file)
        for r in ours:
            print(f"    {r['size']:>6}x{r['size']}: {r['mean_ms']:>10.2f} Â± {r['std_ms']:.2f} ms")
    else:
        ours = []
    remove_if_exists(ours_script)

    # â”€â”€ æ±‡æ€» â”€â”€
    comp = []
    for s, n, o in zip(single, nccl or single, ours or single):
        sp_n = s["mean_ms"]/n["mean_ms"] if n["mean_ms"]>0 else 0
        sp_o = s["mean_ms"]/o["mean_ms"] if o["mean_ms"]>0 else 0
        comp.append({"size":s["size"], "data_gb": round(3*s["size"]**2*4/1024**3,3),
                     "single_ms":s["mean_ms"], "nccl_ms":n["mean_ms"], "ours_ms":o["mean_ms"],
                     "nccl_speedup":round(sp_n,3), "ours_speedup":round(sp_o,3),
                     "ours_div_nccl":round(o["mean_ms"]/n["mean_ms"],2) if n["mean_ms"]>0 else 0})
    data = {"experiment":"çŸ©é˜µä¹˜æ³•è·¨æ¡†æ¶å¯¹æ¯”","gpu_count":world_size,
            "single_gpu":single,"torch_distributed_nccl":nccl,
            "distributed_gpu_framework":ours,"comparison":comp}

    print(f"\n  {'Size':>6} {'æ•°æ®é‡':>8} {'å•GPU':>10} {'NCCL':>10} {'æœ¬æ¡†æ¶':>10} {'NCCLåŠ é€Ÿ':>9} {'æœ¬æ¡†æ¶åŠ é€Ÿ':>9} {'æœ¬/NCCL':>8}")
    for c in comp:
        print(f"  {c['size']:>6} {c['data_gb']:>6.3f}GB {c['single_ms']:>8.2f}ms "
              f"{c['nccl_ms']:>8.2f}ms {c['ours_ms']:>8.2f}ms "
              f"{c['nccl_speedup']:>8.3f}x {c['ours_speedup']:>8.3f}x {c['ours_div_nccl']:>7.2f}x")
    save_json(data, os.path.join(output_dir, "comparison_matmul.json"))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  å®éªŒ 2: AllReduce â€” æœ¬æ¡†æ¶ vs NCCL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def exp_allreduce(gpu_ids, output_dir):
    print("\n" + "=" * 70)
    print("  å®éªŒ: AllReduce â€” æœ¬æ¡†æ¶ vs PyTorch Distributed (NCCL)")
    print("  è§„æ¨¡: ä»1MBå¼€å§‹2å€é€’å¢, æ˜¾å­˜ä¸è¶³æ—¶è‡ªåŠ¨åœæ­¢")
    print("=" * 70)

    world_size = len(gpu_ids)
    free_gb = get_min_free_gb(gpu_ids)
    # allreduce: 1ä¸ªå¼ é‡ + 1ä¸ªæ¥æ”¶ç¼“å†² â‰ˆ 2x
    sizes_mb = []
    mb = 1
    while mb * 2 / 1024 < free_gb * 0.8:
        sizes_mb.append(mb)
        mb *= 2
    print(f"  GPU: {gpu_ids} ({world_size}å¡) | å¯ç”¨æ˜¾å­˜: {free_gb:.1f}GB")
    print(f"  è§„æ¨¡åºåˆ— (MB): {sizes_mb}")
    if not sizes_mb:
        print("  âš  æ˜¾å­˜ä¸è¶³ï¼Œè·³è¿‡"); return

    rep = 10  # AllReduce æ³¢åŠ¨å°ï¼Œå¤šæµ‹å‡ æ¬¡

    # â”€â”€ NCCL â”€â”€
    nccl_file = os.path.join(RESULTS_ROOT, "_tmp_nccl_ar.json")
    nccl_script = os.path.join(RESULTS_ROOT, "_tmp_nccl_ar.py")
    with open(nccl_script, "w") as f:
        f.write(f'''
import os,json,time; import numpy as np; import torch,torch.distributed
rank=int(os.environ["RANK"]); lr=int(os.environ.get("LOCAL_RANK",rank))
torch.cuda.set_device(lr); torch.distributed.init_process_group("nccl")
sizes_mb={sizes_mb}; rep={rep}; results=[]; TLIMIT={PER_SIZE_TIMEOUT}
for mb in sizes_mb:
    sz_t0=time.perf_counter()
    n=mb*1024*1024//4; d=torch.randn(n,device=f"cuda:{{lr}}")
    for _ in range(3): torch.distributed.all_reduce(d); torch.cuda.synchronize(lr)
    ts=[]
    for _ in range(rep):
        torch.cuda.synchronize(lr); torch.distributed.barrier(); t0=time.perf_counter()
        torch.distributed.all_reduce(d)
        torch.cuda.synchronize(lr); torch.distributed.barrier(); t1=time.perf_counter()
        ts.append((t1-t0)*1000)
    if rank==0: results.append({{"size_mb":mb,"mean_ms":round(float(np.mean(ts)),3),"std_ms":round(float(np.std(ts)),3)}})
    del d; torch.cuda.empty_cache()
    if time.perf_counter()-sz_t0>TLIMIT: break
if rank==0:
    with open("{nccl_file}","w") as ff: json.dump(results,ff)
torch.distributed.destroy_process_group()
''')
    env, _ = make_env(gpu_ids)
    print(f"\n  â”€â”€ NCCL AllReduce {world_size}å¡ â”€â”€")
    run_subprocess(["torchrun",f"--nproc_per_node={world_size}","--master_port=29501",nccl_script],
                   env, label="NCCL AllReduce")
    nccl = load_tmp_json(nccl_file)
    for r in nccl: print(f"    {r['size_mb']:>6}MB: {r['mean_ms']:>10.3f} Â± {r['std_ms']:.3f} ms")
    remove_if_exists(nccl_script)

    # â”€â”€ æœ¬æ¡†æ¶ â”€â”€
    ours_file = os.path.join(RESULTS_ROOT, "_tmp_ours_ar.json")
    ours_script = os.path.join(RESULTS_ROOT, "_tmp_ours_ar.py")
    with open(ours_script, "w") as f:
        f.write(f'''
import sys,os,json,time; sys.path.insert(0,"{PROJECT_ROOT}")
import numpy as np; import torch
from distributed_gpu.mpi_manager import MPIManager
mpi=MPIManager(); gid=mpi.get_gpu_id()
sizes_mb={sizes_mb}; rep={rep}; results=[]; TLIMIT={PER_SIZE_TIMEOUT}
for mb in sizes_mb:
    sz_t0=time.perf_counter()
    n=mb*1024*1024//4; d=torch.randn(n,device=f"cuda:{{gid}}")
    for _ in range(3): mpi.allreduce_tensor(d); torch.cuda.synchronize(gid)
    ts=[]
    for _ in range(rep):
        torch.cuda.synchronize(gid); mpi.barrier(); t0=time.perf_counter()
        mpi.allreduce_tensor(d)
        torch.cuda.synchronize(gid); mpi.barrier(); t1=time.perf_counter()
        ts.append((t1-t0)*1000)
    if mpi.is_master_process():
        results.append({{"size_mb":mb,"mean_ms":round(float(np.mean(ts)),3),"std_ms":round(float(np.std(ts)),3)}})
    del d; torch.cuda.empty_cache()
    skip=time.perf_counter()-sz_t0>TLIMIT
    skip=mpi.broadcast(skip if mpi.is_master_process() else None)
    if skip: break
if mpi.is_master_process():
    with open("{ours_file}","w") as ff: json.dump(results,ff)
''')
    print(f"\n  â”€â”€ æœ¬æ¡†æ¶ AllReduce {world_size}å¡ â”€â”€")
    run_subprocess(["mpirun","-n",str(world_size),"--allow-run-as-root","--oversubscribe",
                    sys.executable,ours_script], env, label="æœ¬æ¡†æ¶ AllReduce")
    ours = load_tmp_json(ours_file)
    for r in ours: print(f"    {r['size_mb']:>6}MB: {r['mean_ms']:>10.3f} Â± {r['std_ms']:.3f} ms")
    remove_if_exists(ours_script)

    # â”€â”€ æ±‡æ€» â”€â”€
    comp = []
    for n, o in zip(nccl, ours):
        ratio = o["mean_ms"]/n["mean_ms"] if n["mean_ms"]>0 else 0
        bw_n = n["size_mb"]/n["mean_ms"]*1000/1000 if n["mean_ms"]>0 else 0
        bw_o = o["size_mb"]/o["mean_ms"]*1000/1000 if o["mean_ms"]>0 else 0
        comp.append({"size_mb":n["size_mb"],"nccl_ms":n["mean_ms"],"ours_ms":o["mean_ms"],
                     "ours_div_nccl":round(ratio,2),"nccl_bw_gbps":round(bw_n,2),"ours_bw_gbps":round(bw_o,2)})
    data = {"experiment":"AllReduceè·¨æ¡†æ¶å¯¹æ¯”","gpu_count":world_size,
            "torch_distributed_nccl":nccl,"distributed_gpu_framework":ours,"comparison":comp}

    print(f"\n  {'Size':>6} {'NCCL':>10} {'æœ¬æ¡†æ¶':>10} {'æœ¬/NCCL':>8} {'NCCL BW':>10} {'æœ¬æ¡†æ¶BW':>10}")
    for c in comp:
        print(f"  {c['size_mb']:>4}MB {c['nccl_ms']:>8.3f}ms {c['ours_ms']:>8.3f}ms "
              f"{c['ours_div_nccl']:>7.2f}x {c['nccl_bw_gbps']:>8.2f}GB/s {c['ours_bw_gbps']:>8.2f}GB/s")
    save_json(data, os.path.join(output_dir, "comparison_allreduce.json"))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  å®éªŒ 3: Jacobi è¿­ä»£ â€” æœ¬æ¡†æ¶ GPU vs PETSc/NumPy CPU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def exp_stencil(gpu_ids, output_dir):
    print("\n" + "=" * 70)
    print("  å®éªŒ: Jacobi è¿­ä»£ â€” æœ¬æ¡†æ¶ GPU vs PETSc/NumPy CPU")
    print("  è§„æ¨¡: ç½‘æ ¼ä»128å¼€å§‹2å€é€’å¢, è¿­ä»£100/500æ¬¡, æ˜¾å­˜ä¸è¶³æ—¶è‡ªåŠ¨åœæ­¢")
    print("=" * 70)

    world_size = len(gpu_ids)
    free_gb = get_min_free_gb(gpu_ids)
    # jacobi: grid + rhs + padded + dx2_f â‰ˆ 4*N^2
    grid_sizes = make_doubling_sizes(128, free_gb, 4, lambda N: 4 * N * N)
    iters_list = [100, 500]
    configs = [(g, it) for g in grid_sizes for it in iters_list]
    print(f"  GPU: {gpu_ids} ({world_size}å¡) | å¯ç”¨æ˜¾å­˜: {free_gb:.1f}GB")
    print(f"  ç½‘æ ¼è§„æ¨¡: {grid_sizes}, è¿­ä»£: {iters_list}")
    if not configs:
        print("  âš  æ˜¾å­˜ä¸è¶³ï¼Œè·³è¿‡"); return

    # â”€â”€ CPU åŸºçº¿ (PETSc or NumPy) â”€â”€
    print(f"\n  â”€â”€ CPU åŸºçº¿ (NumPy Jacobi) â”€â”€")
    cpu_results = []
    for grid_sz, iters in configs:
        u = np.random.randn(grid_sz, grid_sz).astype(np.float32)
        f_rhs = np.random.randn(grid_sz, grid_sz).astype(np.float32)
        def numpy_jacobi():
            uu = u.copy()
            for _ in range(iters):
                uu[1:-1,1:-1] = (uu[:-2,1:-1]+uu[2:,1:-1]+uu[1:-1,:-2]+uu[1:-1,2:]-f_rhs[1:-1,1:-1])/4.0
        m, s = benchmark_fn(numpy_jacobi, repeats=3, warmup=1)
        cpu_results.append({"grid":grid_sz,"iterations":iters,"mean_ms":round(m,2),"std_ms":round(s,2)})
        print(f"    {grid_sz:>6}x{grid_sz} x{iters:>3}iter: {m:>10.2f} Â± {s:.2f} ms")

    # PETSc å¦‚æœå¯ç”¨
    petsc_available = False
    try:
        import petsc4py
        petsc_available = True
        print(f"\n  â”€â”€ PETSc (petsc4py {petsc4py.__version__}) Jacobi CPU â”€â”€")
        petsc4py.init(sys.argv[:1])
        from petsc4py import PETSc as _P
        petsc_results = []
        for grid_sz, iters in configs:
            da = _P.DMDA().create(dim=2, sizes=[grid_sz,grid_sz], stencil_width=1,
                                  stencil_type=_P.DMDA.StencilType.STAR)
            u_v=da.createGlobalVec(); f_v=da.createGlobalVec()
            ua=da.getVecArray(u_v); fa=da.getVecArray(f_v)
            rng=np.random.default_rng(42); ua[:]=rng.standard_normal(ua.shape); fa[:]=rng.standard_normal(fa.shape)
            def petsc_jacobi():
                mat=da.createMatrix(); mat.setType("aij"); mat.setFromOptions(); mat.setUp()
                (xs,xe),(ys,ye)=da.getRanges()
                for j in range(ys,ye):
                    for i in range(xs,xe):
                        row=j*grid_sz+i; mat.setValue(row,row,-4.0)
                        if i>0: mat.setValue(row,row-1,1.0)
                        if i<grid_sz-1: mat.setValue(row,row+1,1.0)
                        if j>0: mat.setValue(row,row-grid_sz,1.0)
                        if j<grid_sz-1: mat.setValue(row,row+grid_sz,1.0)
                mat.assemblyBegin(); mat.assemblyEnd()
                ksp=_P.KSP().create(); ksp.setOperators(mat)
                ksp.setType("richardson"); ksp.getPC().setType("jacobi")
                ksp.setTolerances(rtol=1e-10,max_it=iters); ksp.setFromOptions()
                ksp.solve(f_v,u_v); ksp.destroy(); mat.destroy()
            m, s = benchmark_fn(petsc_jacobi, repeats=3, warmup=1)
            petsc_results.append({"grid":grid_sz,"iterations":iters,"mean_ms":round(m,2),"std_ms":round(s,2)})
            print(f"    {grid_sz:>6}x{grid_sz} x{iters:>3}iter: {m:>10.2f} Â± {s:.2f} ms")
            da.destroy(); u_v.destroy(); f_v.destroy()
    except Exception as e:
        print(f"  âš  PETSc ä¸å¯ç”¨: {e}")
        petsc_results = []

    # â”€â”€ æœ¬æ¡†æ¶ GPU â”€â”€
    ours_file = os.path.join(RESULTS_ROOT, "_tmp_ours_jac.json")
    ours_script = os.path.join(RESULTS_ROOT, "_tmp_ours_jac.py")
    with open(ours_script, "w") as f:
        f.write(f'''
import sys,os,json,time,gc; sys.path.insert(0,"{PROJECT_ROOT}")
import numpy as np; import torch
from distributed_gpu.mpi_manager import MPIManager
from distributed_gpu.tensor_distributor import TensorDistributor
from distributed_gpu.algorithms.stencil import distributed_jacobi_2d
mpi=MPIManager(); dist=TensorDistributor(mpi); gid=mpi.get_gpu_id()
configs={configs}; results=[]; TLIMIT={PER_SIZE_TIMEOUT}
for gsz,iters in configs:
    sz_t0=time.perf_counter()
    g=r=None
    if mpi.is_master_process():
        g=torch.randn(gsz,gsz,device=f"cuda:{{gid}}"); r=torch.randn(gsz,gsz,device=f"cuda:{{gid}}")
    for _ in range(1):
        mpi.barrier(); distributed_jacobi_2d(g,r,mpi,dist,iterations=iters,tol=1e-10)
        torch.cuda.synchronize(gid); mpi.barrier(); gc.collect(); torch.cuda.empty_cache()
    ts=[]
    for _ in range(3):
        mpi.barrier(); torch.cuda.synchronize(gid); t0=time.perf_counter()
        distributed_jacobi_2d(g,r,mpi,dist,iterations=iters,tol=1e-10)
        torch.cuda.synchronize(gid); mpi.barrier(); t1=time.perf_counter()
        ts.append((t1-t0)*1000); gc.collect(); torch.cuda.empty_cache()
    if mpi.is_master_process():
        results.append({{"grid":gsz,"iterations":iters,"mean_ms":round(float(np.mean(ts)),2),"std_ms":round(float(np.std(ts)),2)}})
        del g,r
    gc.collect(); torch.cuda.empty_cache(); mpi.barrier()
    skip=time.perf_counter()-sz_t0>TLIMIT
    skip=mpi.broadcast(skip if mpi.is_master_process() else None)
    if skip: break
if mpi.is_master_process():
    with open("{ours_file}","w") as ff: json.dump(results,ff)
''')
    env, _ = make_env(gpu_ids)
    print(f"\n  â”€â”€ æœ¬æ¡†æ¶ åˆ†å¸ƒå¼Jacobi {world_size}å¡ GPU â”€â”€")
    run_subprocess(["mpirun","-n",str(world_size),"--allow-run-as-root","--oversubscribe",
                    sys.executable,ours_script], env, label="æœ¬æ¡†æ¶ Jacobi")
    ours = load_tmp_json(ours_file)
    for r in ours: print(f"    {r['grid']:>6}x{r['grid']} x{r['iterations']:>3}iter: {r['mean_ms']:>10.2f} Â± {r['std_ms']:.2f} ms")
    remove_if_exists(ours_script)

    # â”€â”€ æ±‡æ€» â”€â”€
    comp = []
    cpu_ref = petsc_results if petsc_results else cpu_results
    cpu_label = "petsc" if petsc_results else "numpy"
    for c, o in zip(cpu_ref, ours):
        sp = c["mean_ms"]/o["mean_ms"] if o["mean_ms"]>0 else 0
        comp.append({"grid":c["grid"],"iterations":c["iterations"],
                     f"{cpu_label}_cpu_ms":c["mean_ms"],"gpu_dist_ms":o["mean_ms"],
                     "gpu_speedup_vs_cpu":round(sp,2)})
    data = {"experiment":"Jacobiè¿­ä»£è·¨æ¡†æ¶å¯¹æ¯”","gpu_count":world_size,
            "cpu_baseline":cpu_ref,"cpu_backend":cpu_label,
            "petsc_results":petsc_results,"numpy_results":cpu_results,
            "distributed_gpu_framework":ours,"comparison":comp}

    print(f"\n  {'Grid':>6} {'Iter':>5} {cpu_label.upper()+' CPU':>12} {'æœ¬æ¡†æ¶GPU':>12} {'GPU/CPUåŠ é€Ÿ':>12}")
    for c in comp:
        cpu_k = f"{cpu_label}_cpu_ms"
        print(f"  {c['grid']:>6} {c['iterations']:>5} {c[cpu_k]:>10.2f}ms {c['gpu_dist_ms']:>10.2f}ms {c['gpu_speedup_vs_cpu']:>10.2f}x")
    save_json(data, os.path.join(output_dir, "comparison_jacobi.json"))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  å®éªŒ 4: FFT2D â€” æœ¬æ¡†æ¶ vs Dask-CUDA vs å•GPU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def exp_fft(gpu_ids, output_dir):
    print("\n" + "=" * 70)
    print("  å®éªŒ: FFT2D â€” æœ¬æ¡†æ¶ vs Dask-CUDA + CuPy vs å•GPU")
    print("  è§„æ¨¡: batchä»4å¼€å§‹2å€é€’å¢, gridä»256å¼€å§‹2å€é€’å¢, æ˜¾å­˜ä¸è¶³æ—¶è‡ªåŠ¨åœæ­¢")
    print("=" * 70)

    world_size = len(gpu_ids)
    dev = torch.device(f"cuda:{gpu_ids[0]}")
    free_gb = get_min_free_gb(gpu_ids)
    # fft2d: input(complex128 output doubles size) â‰ˆ batch*grid*grid * 16 bytes
    configs = []
    for grid in [256, 512, 1024, 2048, 4096]:
        batch = 4
        while batch * grid * grid * 16 / 1024**3 < free_gb * 0.6:
            configs.append((batch, grid))
            batch *= 2
    print(f"  GPU: {gpu_ids} ({world_size}å¡) | å¯ç”¨æ˜¾å­˜: {free_gb:.1f}GB")
    print(f"  é…ç½®æ•°: {len(configs)} ç»„")
    for b, g in configs:
        print(f"    batch={b:>4}, grid={g:>5}  ({b*g*g*4/1024**2:.1f} MB)")
    if not configs:
        print("  âš  æ˜¾å­˜ä¸è¶³ï¼Œè·³è¿‡"); return

    # â”€â”€ å•GPU â”€â”€
    print(f"\n  â”€â”€ å•GPU PyTorch FFT2D â”€â”€")
    single = []
    for batch, grid in configs:
        d = torch.randn(batch, grid, grid, device=dev)
        m, s = benchmark_fn(lambda: torch.fft.fft2(d), device=dev)
        single.append({"batch":batch,"grid":grid,"mean_ms":round(m,2),"std_ms":round(s,2)})
        print(f"    batch={batch:>4} grid={grid:>5}: {m:>10.2f} Â± {s:.2f} ms")
        del d; clean()

    # â”€â”€ Dask-CUDA â”€â”€
    print(f"\n  â”€â”€ Dask-CUDA + CuPy FFT2D {world_size}å¡ â”€â”€")
    dask_results = []
    try:
        import cupy as cp
        import dask.array as da
        from dask_cuda import LocalCUDACluster
        from dask.distributed import Client
        phys = get_physical_gpu_ids(gpu_ids)
        cluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=phys, n_workers=world_size)
        client = Client(cluster)
        for batch, grid in configs:
            x = np.random.randn(batch, grid, grid).astype(np.float32)
            x_da = da.from_array(x, chunks=(max(1, batch//world_size), grid, grid))
            def dask_fft(): da.fft.fft2(x_da).compute()
            for _ in range(2): dask_fft()
            ts = []
            for _ in range(REPEATS):
                t0=time.perf_counter(); dask_fft(); t1=time.perf_counter()
                ts.append((t1-t0)*1000)
            m,s = float(np.mean(ts)), float(np.std(ts))
            dask_results.append({"batch":batch,"grid":grid,"mean_ms":round(m,2),"std_ms":round(s,2)})
            print(f"    batch={batch:>4} grid={grid:>5}: {m:>10.2f} Â± {s:.2f} ms")
        client.close(); cluster.close()
    except Exception as e:
        print(f"  âš  Dask-CUDA å¤±è´¥: {e}")
        print(f"  [å›é€€: CuPy å•GPU FFT2D]")
        try:
            import cupy as cp
            for batch, grid in configs:
                d = cp.random.randn(batch, grid, grid, dtype=cp.float32)
                def cupy_fft(): cp.fft.fft2(d); cp.cuda.Stream.null.synchronize()
                m, s = benchmark_fn(cupy_fft)
                dask_results.append({"batch":batch,"grid":grid,"mean_ms":round(m,2),"std_ms":round(s,2),"backend":"cupy_single"})
                print(f"    batch={batch:>4} grid={grid:>5}: {m:>10.2f} Â± {s:.2f} ms")
                del d
        except Exception as e2:
            print(f"  âš  CuPy ä¹Ÿå¤±è´¥: {e2}")

    # â”€â”€ æœ¬æ¡†æ¶ â”€â”€
    ours_file = os.path.join(RESULTS_ROOT, "_tmp_ours_fft.json")
    ours_script = os.path.join(RESULTS_ROOT, "_tmp_ours_fft.py")
    with open(ours_script, "w") as f:
        f.write(f'''
import sys,os,json,time,gc; sys.path.insert(0,"{PROJECT_ROOT}")
import numpy as np; import torch
from distributed_gpu.mpi_manager import MPIManager
from distributed_gpu.tensor_distributor import TensorDistributor
from distributed_gpu.algorithms.fft import distributed_fft2d
mpi=MPIManager(); dist=TensorDistributor(mpi); gid=mpi.get_gpu_id()
configs={configs}; results=[]; TLIMIT={PER_SIZE_TIMEOUT}
for batch,grid in configs:
    sz_t0=time.perf_counter()
    d=None
    if mpi.is_master_process(): d=torch.randn(batch,grid,grid,device=f"cuda:{{gid}}")
    for _ in range(2):
        mpi.barrier(); distributed_fft2d(d,mpi,dist); torch.cuda.synchronize(gid); mpi.barrier()
        gc.collect(); torch.cuda.empty_cache()
    ts=[]
    for _ in range({REPEATS}):
        mpi.barrier(); torch.cuda.synchronize(gid); t0=time.perf_counter()
        distributed_fft2d(d,mpi,dist)
        torch.cuda.synchronize(gid); mpi.barrier(); t1=time.perf_counter()
        ts.append((t1-t0)*1000); gc.collect(); torch.cuda.empty_cache()
    if mpi.is_master_process():
        results.append({{"batch":batch,"grid":grid,"mean_ms":round(float(np.mean(ts)),2),"std_ms":round(float(np.std(ts)),2)}})
        del d
    gc.collect(); torch.cuda.empty_cache(); mpi.barrier()
    skip=time.perf_counter()-sz_t0>TLIMIT
    skip=mpi.broadcast(skip if mpi.is_master_process() else None)
    if skip: break
if mpi.is_master_process():
    with open("{ours_file}","w") as ff: json.dump(results,ff)
''')
    env, _ = make_env(gpu_ids)
    print(f"\n  â”€â”€ æœ¬æ¡†æ¶ åˆ†å¸ƒå¼FFT2D {world_size}å¡ â”€â”€")
    run_subprocess(["mpirun","-n",str(world_size),"--allow-run-as-root","--oversubscribe",
                    sys.executable,ours_script], env, label="æœ¬æ¡†æ¶ FFT2D")
    ours = load_tmp_json(ours_file)
    for r in ours: print(f"    batch={r['batch']:>4} grid={r['grid']:>5}: {r['mean_ms']:>10.2f} Â± {r['std_ms']:.2f} ms")
    remove_if_exists(ours_script)

    # â”€â”€ æ±‡æ€» â”€â”€
    comp = []
    for i, (s, o) in enumerate(zip(single, ours)):
        d = dask_results[i] if i < len(dask_results) else {"mean_ms":0}
        sp_o = s["mean_ms"]/o["mean_ms"] if o["mean_ms"]>0 else 0
        sp_d = s["mean_ms"]/d["mean_ms"] if d["mean_ms"]>0 else 0
        comp.append({"batch":s["batch"],"grid":s["grid"],"data_mb":round(s["batch"]*s["grid"]**2*4/1024**2,1),
                     "single_ms":s["mean_ms"],"dask_ms":d["mean_ms"],"ours_ms":o["mean_ms"],
                     "ours_vs_single":round(sp_o,3),"dask_vs_single":round(sp_d,3)})
    data = {"experiment":"FFT2Dè·¨æ¡†æ¶å¯¹æ¯”","gpu_count":world_size,
            "single_gpu":single,"dask_cuda":dask_results,
            "distributed_gpu_framework":ours,"comparison":comp}

    print(f"\n  {'Batch':>5} {'Grid':>5} {'Data':>7} {'å•GPU':>10} {'Dask':>10} {'æœ¬æ¡†æ¶':>10} {'æœ¬/å•':>8} {'Dask/å•':>8}")
    for c in comp:
        print(f"  {c['batch']:>5} {c['grid']:>5} {c['data_mb']:>5.1f}MB {c['single_ms']:>8.2f}ms "
              f"{c['dask_ms']:>8.2f}ms {c['ours_ms']:>8.2f}ms {c['ours_vs_single']:>7.3f}x {c['dask_vs_single']:>7.3f}x")
    save_json(data, os.path.join(output_dir, "comparison_fft.json"))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  å®éªŒ 5: å½’çº¦ (Sum) â€” æœ¬æ¡†æ¶ vs NCCL vs å•GPU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def exp_reduction(gpu_ids, output_dir):
    print("\n" + "=" * 70)
    print("  å®éªŒ: å…¨å±€æ±‚å’Œ (Sum) â€” æœ¬æ¡†æ¶ vs NCCL vs å•GPU")
    print("  è§„æ¨¡: NxN, Nä»1024å¼€å§‹2å€é€’å¢, æ˜¾å­˜ä¸è¶³æ—¶è‡ªåŠ¨åœæ­¢")
    print("=" * 70)

    world_size = len(gpu_ids)
    dev = torch.device(f"cuda:{gpu_ids[0]}")
    free_gb = get_min_free_gb(gpu_ids)
    sizes = make_doubling_sizes(1024, free_gb, 4, lambda N: N * N * 2)
    print(f"  GPU: {gpu_ids} ({world_size}å¡) | å¯ç”¨æ˜¾å­˜: {free_gb:.1f}GB")
    print(f"  è§„æ¨¡åºåˆ—: {sizes}")
    if not sizes:
        print("  âš  æ˜¾å­˜ä¸è¶³ï¼Œè·³è¿‡"); return

    # â”€â”€ å•GPU â”€â”€
    print(f"\n  â”€â”€ å•GPU torch.sum â”€â”€")
    single = []
    for N in sizes:
        d = torch.randn(N, N, device=dev)
        m, s = benchmark_fn(lambda: torch.sum(d), device=dev)
        single.append({"size":N,"mean_ms":round(m,2),"std_ms":round(s,2)})
        print(f"    {N:>6}x{N}: {m:>10.2f} Â± {s:.2f} ms")
        del d; clean()

    # â”€â”€ NCCL reduce â”€â”€
    nccl_file = os.path.join(RESULTS_ROOT, "_tmp_nccl_sum.json")
    nccl_script = os.path.join(RESULTS_ROOT, "_tmp_nccl_sum.py")
    with open(nccl_script, "w") as f:
        f.write(f'''
import os,json,time; import numpy as np; import torch,torch.distributed
rank=int(os.environ["RANK"]); lr=int(os.environ.get("LOCAL_RANK",rank))
torch.cuda.set_device(lr); torch.distributed.init_process_group("nccl")
ws=int(os.environ["WORLD_SIZE"]); sizes={sizes}; rep={REPEATS}; results=[]; TLIMIT={PER_SIZE_TIMEOUT}
for N in sizes:
    sz_t0=time.perf_counter()
    ck=N//ws; d=torch.randn(ck,N,device=f"cuda:{{lr}}")
    for _ in range(2):
        s=d.sum(); torch.distributed.all_reduce(s); torch.cuda.synchronize(lr)
    ts=[]
    for _ in range(rep):
        torch.cuda.synchronize(lr); torch.distributed.barrier(); t0=time.perf_counter()
        s=d.sum(); torch.distributed.all_reduce(s)
        torch.cuda.synchronize(lr); torch.distributed.barrier(); t1=time.perf_counter()
        ts.append((t1-t0)*1000)
    if rank==0: results.append({{"size":N,"mean_ms":round(float(np.mean(ts)),2),"std_ms":round(float(np.std(ts)),2)}})
    del d; torch.cuda.empty_cache()
    if time.perf_counter()-sz_t0>TLIMIT: break
if rank==0:
    with open("{nccl_file}","w") as ff: json.dump(results,ff)
torch.distributed.destroy_process_group()
''')
    env, _ = make_env(gpu_ids)
    print(f"\n  â”€â”€ NCCL Reduce {world_size}å¡ â”€â”€")
    run_subprocess(["torchrun",f"--nproc_per_node={world_size}","--master_port=29502",nccl_script],
                   env, label="NCCL Sum")
    nccl = load_tmp_json(nccl_file)
    for r in nccl: print(f"    {r['size']:>6}x{r['size']}: {r['mean_ms']:>10.2f} Â± {r['std_ms']:.2f} ms")
    remove_if_exists(nccl_script)

    # â”€â”€ æœ¬æ¡†æ¶ â”€â”€
    ours_file = os.path.join(RESULTS_ROOT, "_tmp_ours_sum.json")
    ours_script = os.path.join(RESULTS_ROOT, "_tmp_ours_sum.py")
    with open(ours_script, "w") as f:
        f.write(f'''
import sys,os,json,time,gc; sys.path.insert(0,"{PROJECT_ROOT}")
import numpy as np; import torch
from distributed_gpu.mpi_manager import MPIManager
from distributed_gpu.tensor_distributor import TensorDistributor
from distributed_gpu.algorithms.reduction import distributed_sum
mpi=MPIManager(); dist=TensorDistributor(mpi); gid=mpi.get_gpu_id()
sizes={sizes}; rep={REPEATS}; results=[]; TLIMIT={PER_SIZE_TIMEOUT}
for N in sizes:
    sz_t0=time.perf_counter()
    d=None
    if mpi.is_master_process(): d=torch.randn(N,N,device=f"cuda:{{gid}}")
    for _ in range(2):
        mpi.barrier(); distributed_sum(d,mpi,dist); torch.cuda.synchronize(gid); mpi.barrier()
        gc.collect(); torch.cuda.empty_cache()
    ts=[]
    for _ in range(rep):
        mpi.barrier(); torch.cuda.synchronize(gid); t0=time.perf_counter()
        distributed_sum(d,mpi,dist)
        torch.cuda.synchronize(gid); mpi.barrier(); t1=time.perf_counter()
        ts.append((t1-t0)*1000); gc.collect(); torch.cuda.empty_cache()
    if mpi.is_master_process():
        results.append({{"size":N,"mean_ms":round(float(np.mean(ts)),2),"std_ms":round(float(np.std(ts)),2)}})
        del d
    gc.collect(); torch.cuda.empty_cache(); mpi.barrier()
    skip=time.perf_counter()-sz_t0>TLIMIT
    skip=mpi.broadcast(skip if mpi.is_master_process() else None)
    if skip: break
if mpi.is_master_process():
    with open("{ours_file}","w") as ff: json.dump(results,ff)
''')
    print(f"\n  â”€â”€ æœ¬æ¡†æ¶ åˆ†å¸ƒå¼Sum {world_size}å¡ â”€â”€")
    run_subprocess(["mpirun","-n",str(world_size),"--allow-run-as-root","--oversubscribe",
                    sys.executable,ours_script], env, label="æœ¬æ¡†æ¶ Sum")
    ours = load_tmp_json(ours_file)
    for r in ours: print(f"    {r['size']:>6}x{r['size']}: {r['mean_ms']:>10.2f} Â± {r['std_ms']:.2f} ms")
    remove_if_exists(ours_script)

    # â”€â”€ æ±‡æ€» â”€â”€
    comp = []
    for s, n, o in zip(single, nccl or single, ours or single):
        sp_n = s["mean_ms"]/n["mean_ms"] if n["mean_ms"]>0 else 0
        sp_o = s["mean_ms"]/o["mean_ms"] if o["mean_ms"]>0 else 0
        comp.append({"size":s["size"],"data_gb":round(s["size"]**2*4/1024**3,3),
                     "single_ms":s["mean_ms"],"nccl_ms":n["mean_ms"],"ours_ms":o["mean_ms"],
                     "nccl_speedup":round(sp_n,3),"ours_speedup":round(sp_o,3)})
    data = {"experiment":"å…¨å±€æ±‚å’Œè·¨æ¡†æ¶å¯¹æ¯”","gpu_count":world_size,
            "single_gpu":single,"torch_distributed_nccl":nccl,
            "distributed_gpu_framework":ours,"comparison":comp}
    save_json(data, os.path.join(output_dir, "comparison_reduction.json"))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ä¸»å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXPERIMENTS = {
    "matmul":    ("çŸ©é˜µä¹˜æ³•: æœ¬æ¡†æ¶ vs NCCL vs å•GPU (2å€é€’å¢)", exp_matmul),
    "allreduce": ("AllReduce: æœ¬æ¡†æ¶ vs NCCL (2å€é€’å¢)", exp_allreduce),
    "stencil":   ("Jacobiè¿­ä»£: æœ¬æ¡†æ¶GPU vs PETSc/NumPy CPU (2å€é€’å¢)", exp_stencil),
    "fft":       ("FFT2D: æœ¬æ¡†æ¶ vs Dask-CUDA vs å•GPU (2å€é€’å¢)", exp_fft),
    "reduction": ("å…¨å±€æ±‚å’Œ: æœ¬æ¡†æ¶ vs NCCL vs å•GPU (2å€é€’å¢)", exp_reduction),
}


def main():
    parser = argparse.ArgumentParser(
        description="è·¨æ¡†æ¶æ€§èƒ½å¯¹æ¯”å®éªŒ (è‡ªé€‚åº”è§„æ¨¡ï¼Œ2å€é€’å¢ï¼Œè¶…æ˜¾å­˜è‡ªåŠ¨åœæ­¢)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python experiments/benchmark_comparison.py                    # å…¨éƒ¨å®éªŒ
  python experiments/benchmark_comparison.py --gpus 4           # 4å¡
  python experiments/benchmark_comparison.py --exp matmul       # åªè·‘çŸ©é˜µä¹˜æ³•
  python experiments/benchmark_comparison.py --exp matmul,fft   # è·‘ä¸¤ä¸ª
  python experiments/benchmark_comparison.py --list             # æŸ¥çœ‹åˆ—è¡¨

æ³¨æ„: ä¸è¦ç”¨ mpirun å¯åŠ¨æœ¬è„šæœ¬ï¼è„šæœ¬å†…éƒ¨ä¼šè‡ªè¡Œç®¡ç† MPI/torchrun å­è¿›ç¨‹ã€‚
""")
    parser.add_argument("--gpus", "-g", type=int, default=None,
                        help="ä½¿ç”¨çš„GPUæ•° (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹å…¨éƒ¨ç©ºé—²GPU)")
    parser.add_argument("--exp", "-e", type=str, default="all",
                        help="å®éªŒå (é€—å·åˆ†éš”) æˆ– all")
    parser.add_argument("--list", "-l", action="store_true", help="æŸ¥çœ‹å¯ç”¨å®éªŒ")
    args = parser.parse_args()

    if args.list:
        print("\nå¯ç”¨å¯¹æ¯”å®éªŒ:")
        print("-" * 65)
        for k, (desc, _) in EXPERIMENTS.items():
            print(f"  {k:<12} {desc}")
        print(f"  {'all':<12} è¿è¡Œå…¨éƒ¨ ({len(EXPERIMENTS)} ä¸ªå®éªŒ)")
        print("-" * 65)
        return

    free_gpus = detect_free_gpus(min_free_mb=10000)
    if args.gpus:
        free_gpus = free_gpus[:args.gpus]
    if len(free_gpus) < 2:
        print(f"âŒ éœ€è¦è‡³å°‘ 2 ä¸ªç©ºé—² GPU (â‰¥10GBç©ºé—²)ï¼Œå½“å‰å¯ç”¨: {free_gpus}")
        print("   æç¤º: è®¾ç½® CUDA_VISIBLE_DEVICES æŒ‡å®šç©ºé—²GPU")
        print("   ä¾‹: CUDA_VISIBLE_DEVICES=1,3,5 python experiments/benchmark_comparison.py")
        return

    free_gb = get_min_free_gb(free_gpus)
    print(f"\nğŸš€ è·¨æ¡†æ¶å¯¹æ¯”å®éªŒ")
    print(f"   GPU: {free_gpus} ({len(free_gpus)} å¡)")
    print(f"   æœ€å°å¯ç”¨æ˜¾å­˜: {free_gb:.1f} GB")
    print(f"   è§„æ¨¡ç­–ç•¥: 2å€é€’å¢, è¶…æ˜¾å­˜è‡ªåŠ¨åœæ­¢")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(RESULTS_ROOT, f"comparison_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)
    print(f"   ç»“æœç›®å½•: {output_dir}")

    if args.exp == "all":
        run_list = list(EXPERIMENTS.keys())
    else:
        run_list = [x.strip() for x in args.exp.split(",")]

    t_start = time.time()
    for exp_name in run_list:
        if exp_name not in EXPERIMENTS:
            print(f"\n  âš  æœªçŸ¥å®éªŒ: {exp_name}, å¯ç”¨: {list(EXPERIMENTS.keys())}")
            continue
        _, fn = EXPERIMENTS[exp_name]
        try:
            fn(free_gpus, output_dir)
        except Exception as e:
            print(f"\n  âŒ å®éªŒ {exp_name} å¼‚å¸¸: {e}")
            import traceback; traceback.print_exc()

    elapsed = time.time() - t_start
    print(f"\nâœ… å®éªŒå®Œæˆ! è€—æ—¶: {elapsed:.0f}s ({elapsed/60:.1f}min)")
    print(f"   ç»“æœç›®å½•: {output_dir}")
    print(f"   æ–‡ä»¶åˆ—è¡¨:")
    for f in sorted(os.listdir(output_dir)):
        fpath = os.path.join(output_dir, f)
        print(f"     {f} ({os.path.getsize(fpath)/1024:.1f} KB)")

    # â”€â”€ è‡ªåŠ¨ç”Ÿæˆå›¾è¡¨ â”€â”€
    print(f"\nğŸ“Š ç”Ÿæˆè·¨æ¡†æ¶å¯¹æ¯”å›¾è¡¨...")
    try:
        fig_script = os.path.join(SCRIPT_DIR, "generate_thesis_figures_enhanced.py")
        if os.path.exists(fig_script):
            subprocess.run([sys.executable, fig_script, "--data-dir", output_dir],
                           timeout=120)
            fig_dir = os.path.join(output_dir, "figures")
            if os.path.isdir(fig_dir):
                pngs = [f for f in os.listdir(fig_dir) if f.endswith('.png')]
                print(f"   å›¾è¡¨ç”Ÿæˆå®Œæˆ: {len(pngs)} å¼  â†’ {fig_dir}")
    except Exception as e:
        print(f"   âš  å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")


if __name__ == "__main__":
    main()
