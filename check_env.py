#!/usr/bin/env python3
"""
ç¯å¢ƒæ£€æµ‹è„šæœ¬ - éªŒè¯ distributed_gpu æ¡†æ¶å®‰è£…æ˜¯å¦æˆåŠŸ
ä½¿ç”¨æ–¹å¼: python check_env.py
"""

import sys
import shutil

def check(name, func):
    """æ‰§è¡Œå•é¡¹æ£€æŸ¥"""
    try:
        result = func()
        print(f"  âœ… {name}: {result}")
        return True
    except Exception as e:
        print(f"  âŒ {name}: {e}")
        return False

def main():
    print("=" * 56)
    print("  distributed_gpu ç¯å¢ƒæ£€æµ‹")
    print("=" * 56)
    passed = 0
    total = 0

    # ========== 1. Python ç‰ˆæœ¬ ==========
    print("\n[1/6] Python ç¯å¢ƒ")
    total += 1
    if check("Python ç‰ˆæœ¬", lambda: f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"):
        if sys.version_info >= (3, 8):
            passed += 1
        else:
            print("       âš ï¸  éœ€è¦ Python >= 3.8")

    # ========== 2. PyTorch + CUDA ==========
    print("\n[2/6] PyTorch + CUDA")
    torch_ok = False
    total += 1
    try:
        import torch
        print(f"  âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            cuda_ver = torch.version.cuda
            print(f"  âœ… CUDA ç‰ˆæœ¬: {cuda_ver}")
            print(f"  âœ… GPU æ•°é‡: {gpu_count}")
            print(f"  âœ… GPU å‹å·: {gpu_name}")
            mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"  âœ… GPU 0 æ˜¾å­˜: {mem_total:.1f} GB")
            torch_ok = True
            passed += 1
        else:
            print("  âŒ CUDA ä¸å¯ç”¨! torch.cuda.is_available() = False")
            print("     ä¿®å¤: pip install torch --index-url https://download.pytorch.org/whl/cu121")
    except ImportError:
        print("  âŒ PyTorch æœªå®‰è£…!")
        print("     ä¿®å¤: pip install torch")

    # ========== 3. MPI ==========
    print("\n[3/6] MPI ç¯å¢ƒ")
    total += 1
    mpi_ok = False
    # æ£€æŸ¥ mpirun å‘½ä»¤
    mpirun_path = shutil.which("mpirun") or shutil.which("mpiexec")
    if mpirun_path:
        print(f"  âœ… MPI è¿è¡Œæ—¶: {mpirun_path}")
    else:
        print("  âŒ mpirun/mpiexec æœªæ‰¾åˆ°!")
        print("     ä¿®å¤: conda install -c conda-forge openmpi -y")

    try:
        from mpi4py import MPI
        print(f"  âœ… mpi4py ç‰ˆæœ¬: {MPI.Get_version()}")
        mpi_ok = True
        if mpirun_path:
            passed += 1
    except ImportError:
        print("  âŒ mpi4py æœªå®‰è£…!")
        print("     ä¿®å¤: conda install -c conda-forge mpi4py -y")

    # ========== 4. å…¶ä»–ä¾èµ– ==========
    print("\n[4/6] å…¶ä»–ä¾èµ–")
    total += 1
    deps_ok = True
    for pkg, import_name in [("numpy", "numpy"), ("opt-einsum", "opt_einsum")]:
        try:
            mod = __import__(import_name)
            ver = getattr(mod, "__version__", "unknown")
            print(f"  âœ… {pkg}: {ver}")
        except ImportError:
            print(f"  âŒ {pkg} æœªå®‰è£…!  ä¿®å¤: pip install {pkg}")
            deps_ok = False
    if deps_ok:
        passed += 1

    # ========== 5. æ¡†æ¶å¯¼å…¥ ==========
    print("\n[5/6] distributed_gpu æ¡†æ¶")
    total += 1
    try:
        import distributed_gpu
        print(f"  âœ… æ¡†æ¶ç‰ˆæœ¬: {distributed_gpu.__version__}")
        print(f"  âœ… å®‰è£…è·¯å¾„: {distributed_gpu.__file__}")

        from distributed_gpu import (
            MPIManager, TensorDistributor, CostModel,
            GPUManager, PipelineOptimizer, ResourcePlanner, AutoExecutor
        )
        print("  âœ… æ ¸å¿ƒæ¨¡å—: MPIManager, TensorDistributor, CostModel ...")

        from distributed_gpu.algorithms import (
            distributed_matmul, distributed_conv2d, distributed_fft,
            distributed_einsum, distributed_sum, distributed_stencil_2d
        )
        print("  âœ… ç®—å­æ¨¡å—: matmul, conv2d, fft, einsum, sum, stencil ...")
        passed += 1
    except ImportError as e:
        print(f"  âŒ æ¡†æ¶å¯¼å…¥å¤±è´¥: {e}")
        print("     ä¿®å¤: cd distributed_gpu && pip install -e .")

    # ========== 6. MPI å¤šè¿›ç¨‹æµ‹è¯•å»ºè®® ==========
    print("\n[6/6] MPI å¤šè¿›ç¨‹æµ‹è¯•")
    total += 1
    if mpi_ok and torch_ok:
        print("  â„¹ï¸  å•è¿›ç¨‹æ£€æµ‹å·²å…¨éƒ¨é€šè¿‡ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯å¤šGPUååŒ:")
        print("     mpirun -n 4 python examples/run_algorithm.py all")
        print("     é¢„æœŸè¾“å‡º: æ€»è®¡: 17/17 é€šè¿‡")
        passed += 1
    else:
        print("  âš ï¸  è¯·å…ˆä¿®å¤ä¸Šè¿°é—®é¢˜åå†æµ‹è¯•å¤šGPUååŒ")

    # ========== æ€»ç»“ ==========
    print("\n" + "=" * 56)
    if passed == total:
        print(f"  ğŸ‰ å…¨éƒ¨é€šè¿‡ ({passed}/{total})ï¼Œç¯å¢ƒé…ç½®æ­£ç¡®ï¼")
    else:
        print(f"  âš ï¸  é€šè¿‡ {passed}/{total}ï¼Œè¯·æ ¹æ®ä¸Šæ–¹æç¤ºä¿®å¤é—®é¢˜")
    print("=" * 56)

    return 0 if passed == total else 1

if __name__ == "__main__":
    sys.exit(main())
