# åŸºäºMPIçš„åˆ†å¸ƒå¼GPUè®¡ç®—æ¡†æ¶

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“– é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªé«˜æ€§èƒ½çš„åˆ†å¸ƒå¼GPUè®¡ç®—æ¡†æ¶ï¼Œç”¨äºå¤„ç†å¤§è§„æ¨¡å¼ é‡çš„ç§‘å­¦è®¡ç®—ä»»åŠ¡ã€‚é€šè¿‡MPIè¿›è¡ŒèŠ‚ç‚¹é—´é€šä¿¡ï¼Œè‡ªåŠ¨å°†å¼ é‡åˆ‡å‰²å¹¶åˆ†é…åˆ°å„ä¸ªGPUèŠ‚ç‚¹ä¸Šå¹¶è¡Œè®¡ç®—ï¼Œçªç ´å•å¡æ˜¾å­˜é™åˆ¶å¹¶æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ã€‚

## âœ¨ æ ¸å¿ƒåˆ›æ–°ç‚¹

### ğŸ¯ åˆ›æ–°ç‚¹1ï¼šåŸºäºä»£ä»·æ¨¡å‹çš„è‡ªé€‚åº”å¼ é‡åˆ†å‰²ç®—æ³•
- å»ºç«‹è®¡ç®—ä»£ä»·ã€é€šä¿¡ä»£ä»·å’Œæ˜¾å­˜ä»£ä»·çš„è”åˆä¼˜åŒ–æ¨¡å‹
- æå‡ºå¤šç›®æ ‡ä¼˜åŒ–çš„åˆ†å‰²ç­–ç•¥é€‰æ‹©ç®—æ³•
- æ”¯æŒå¼‚æ„é›†ç¾¤çš„è‡ªé€‚åº”è´Ÿè½½å‡è¡¡

### ğŸ¯ åˆ›æ–°ç‚¹2ï¼šè®¡ç®—-é€šä¿¡é‡å çš„æµæ°´çº¿ä¼˜åŒ–ç­–ç•¥
- æ¨å¯¼æœ€ä¼˜åˆ†å—å¤§å°çš„è§£æè¡¨è¾¾å¼
- å¤šçº§æµæ°´çº¿å®ç°è®¡ç®—-é€šä¿¡é‡å 
- ç†è®ºåŠ é€Ÿæ¯”æœ€é«˜å¯è¾¾2x

### ğŸ¯ åˆ›æ–°ç‚¹3ï¼šé›†æˆopt_einsumçš„åˆ†å¸ƒå¼å¼ é‡æ”¶ç¼©
- æœ€ä¼˜æ”¶ç¼©è·¯å¾„è‡ªåŠ¨è®¡ç®—
- æ”¯æŒå¤šç§ä¼˜åŒ–ç­–ç•¥ï¼ˆoptimal/dp/greedyï¼‰
- è·¯å¾„ä¼˜åŒ–ä¸åˆ†å¸ƒå¼å¹¶è¡ŒåŒé‡åŠ é€Ÿ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/sxl19971024/distributed_gpu.git
cd distributed_gpu

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æˆ–ä½¿ç”¨ pip å®‰è£…
pip install -e .
```

### è¿è¡Œæµ‹è¯•

```bash
# ä½¿ç”¨4ä¸ªGPUè¿è¡Œç»¼åˆæµ‹è¯•
mpirun -n 4 --allow-run-as-root python examples/test_all.py
```

### åŸºæœ¬ä½¿ç”¨

```python
import torch
from src.mpi_manager import MPIManager
from src.tensor_distributor import TensorDistributor
from src.algorithms.matrix_ops import distributed_matmul

# åˆå§‹åŒ–
mpi = MPIManager()
distributor = TensorDistributor(mpi)

# åˆ›å»ºæ•°æ®ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
if mpi.is_master_process():
    A = torch.randn(10000, 10000).cuda()
    B = torch.randn(10000, 10000).cuda()
else:
    A, B = None, None

# åˆ†å¸ƒå¼è®¡ç®—ï¼ˆæ‰€æœ‰è¿›ç¨‹éƒ½è°ƒç”¨ï¼‰
C = distributed_matmul(A, B, mpi, distributor)

# ç»“æœä»…åœ¨ä¸»è¿›ç¨‹
if mpi.is_master_process():
    print(f"ç»“æœå½¢çŠ¶: {C.shape}")
```

## ğŸ“š æ”¯æŒçš„ç®—æ³•

| ç±»åˆ« | ç®—æ³• | å‡½æ•° |
|------|------|------|
| **çŸ©é˜µè¿ç®—** | çŸ©é˜µä¹˜æ³• | `distributed_matmul` |
| | æ‰¹é‡çŸ©é˜µä¹˜æ³• | `distributed_batch_matmul` |
| | çŸ©é˜µè½¬ç½® | `distributed_transpose` |
| | å¼ é‡åŠ æ³• | `distributed_add` |
| **å·ç§¯æ“ä½œ** | 2Då·ç§¯ | `distributed_conv2d` |
| | 3Då·ç§¯ | `distributed_conv3d` |
| **å‚…é‡Œå¶å˜æ¢** | 1D FFT | `distributed_fft` |
| | 1D IFFT | `distributed_ifft` |
| | 2D FFT | `distributed_fft2d` |
| | å®æ•°FFT | `distributed_rfft` |
| **å¼ é‡æ”¶ç¼©** | Einsteinæ±‚å’Œ | `distributed_einsum` |
| | å¼ é‡ç‚¹ç§¯ | `distributed_tensordot` |
| **å½’çº¦æ“ä½œ** | æ±‚å’Œ | `distributed_sum` |
| | å‡å€¼ | `distributed_mean` |
| | æœ€å¤§å€¼ | `distributed_max` |
| | æœ€å°å€¼ | `distributed_min` |

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
distributed_gpu_framework/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mpi_manager.py          # MPIé€šä¿¡ç®¡ç†å™¨
â”‚   â”œâ”€â”€ tensor_distributor.py   # å¼ é‡åˆ†é…å™¨
â”‚   â”œâ”€â”€ gpu_manager.py          # GPUè®¾å¤‡ç®¡ç†
â”‚   â”œâ”€â”€ cost_model.py           # ä»£ä»·æ¨¡å‹ï¼ˆåˆ›æ–°ç‚¹1ï¼‰
â”‚   â”œâ”€â”€ pipeline_optimizer.py   # æµæ°´çº¿ä¼˜åŒ–ï¼ˆåˆ›æ–°ç‚¹2ï¼‰
â”‚   â”œâ”€â”€ algorithms/             # åˆ†å¸ƒå¼ç®—æ³•åº“
â”‚   â”‚   â”œâ”€â”€ matrix_ops.py       # çŸ©é˜µè¿ç®—
â”‚   â”‚   â”œâ”€â”€ convolution.py      # å·ç§¯æ“ä½œ
â”‚   â”‚   â”œâ”€â”€ fft.py              # å‚…é‡Œå¶å˜æ¢
â”‚   â”‚   â”œâ”€â”€ einsum.py           # Einsteinæ±‚å’Œï¼ˆé›†æˆopt_einsumï¼‰
â”‚   â”‚   â””â”€â”€ reduction.py        # å½’çº¦æ“ä½œ
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ profiler.py         # æ€§èƒ½åˆ†æå·¥å…·
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ test_all.py             # ç»¼åˆæµ‹è¯•
â”‚   â””â”€â”€ matrix_multiplication.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ INSTALL.md                  # å®‰è£…æŒ‡å—
â””â”€â”€ README.md
```

## ğŸ’» ç³»ç»Ÿè¦æ±‚

- Python 3.8+
- CUDA 11.0+
- PyTorch 2.0+
- OpenMPI 4.0+ æˆ– MPICH
- å¤šä¸ªNVIDIA GPU

## ğŸ“Š æ€§èƒ½æµ‹è¯•

åœ¨ 4Ã—RTX 5090 ä¸Šçš„æµ‹è¯•ç»“æœï¼š

| ç®—æ³• | è§„æ¨¡ | å•GPUæ—¶é—´ | 4GPUæ—¶é—´ | åŠ é€Ÿæ¯” |
|------|------|-----------|----------|--------|
| çŸ©é˜µä¹˜æ³• | 10000Ã—10000 | 30ms | 8ms | 3.75x |
| 2Då·ç§¯ | [32,64,512,512] | 150ms | 42ms | 3.57x |
| 2D FFT | [64,1024,1024] | 25ms | 7ms | 3.57x |

## ğŸ“– æ–‡æ¡£

- [å®‰è£…æŒ‡å—](INSTALL.md)
- [APIä½¿ç”¨æŒ‡å—](docs/API_GUIDE.md) - æ¯ä¸ªç®—æ³•çš„è¯¦ç»†å‚æ•°å’Œç¤ºä¾‹
- [ä½¿ç”¨æ•™ç¨‹](docs/tutorial.md)ï¼ˆå¾…å®Œå–„ï¼‰

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ“§ è”ç³»æ–¹å¼

- ä½œè€…: å­™å°æ—
- Email: 1271364457@qq.com

## ğŸ“š å¼•ç”¨

å¦‚æœæœ¬æ¡†æ¶å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{distributed_gpu_framework,
  author = {å­™å°æ—},
  title = {MPI-based Distributed GPU Computing Framework},
  year = {2026},
  url = {https://github.com/sxl19971024/distributed_gpu}
}
```
