# åŸºäºMPIçš„åˆ†å¸ƒå¼GPUè®¡ç®—æ¡†æ¶

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“– é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªé¢å‘**ç§‘å­¦è®¡ç®—**çš„é«˜æ€§èƒ½åˆ†å¸ƒå¼GPUè®¡ç®—æ¡†æ¶ã€‚é€šè¿‡ MPI è¿›è¡ŒèŠ‚ç‚¹é—´é€šä¿¡ï¼Œè‡ªåŠ¨å°†å¤§è§„æ¨¡å¼ é‡åˆ‡å‰²å¹¶åˆ†é…åˆ°å„ä¸ªGPUèŠ‚ç‚¹ä¸Šå¹¶è¡Œè®¡ç®—ï¼Œçªç ´å•å¡æ˜¾å­˜é™åˆ¶å¹¶æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ã€‚

æ¡†æ¶æä¾› **24 ä¸ªåˆ†å¸ƒå¼ç®—å­ + 2 ä¸ªæµæ°´çº¿æ“ä½œï¼ˆå…± 26 ä¸ªæ“ä½œï¼‰**ï¼Œè¦†ç›–çŸ©é˜µè¿ç®—ã€å·ç§¯ã€FFTã€Einstein æ±‚å’Œã€å½’çº¦æ“ä½œå’Œ Stencil/PDE æ±‚è§£ï¼Œé€‚ç”¨äºç‰©ç†æ¨¡æ‹Ÿã€æ•°å€¼æ±‚è§£ã€å¤§è§„æ¨¡çº¿æ€§ä»£æ•°ç­‰ç§‘å­¦è®¡ç®—åœºæ™¯ã€‚

## âœ¨ æ ¸å¿ƒåˆ›æ–°ç‚¹

### ğŸ¯ åˆ›æ–°ç‚¹1ï¼šåŸºäºä»£ä»·æ¨¡å‹çš„è‡ªé€‚åº”å¼ é‡åˆ†å‰²ç®—æ³•
- å»ºç«‹äº† **è®¡ç®—ä»£ä»· + é€šä¿¡ä»£ä»· + æ˜¾å­˜ä»£ä»·** çš„è”åˆä¼˜åŒ–æ¨¡å‹
- æ”¯æŒä¸‰ç§åˆ†å‰²ç­–ç•¥å¹¶è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆï¼š
  - **è¡Œåˆ†å‰²ï¼ˆRow Splitï¼‰**ï¼šæŒ‰ A çš„è¡Œåˆ†å‰²ï¼Œé€‚åˆ M â‰« N
  - **åˆ—åˆ†å‰²ï¼ˆColumn Splitï¼‰**ï¼šæŒ‰ B çš„åˆ—åˆ†å‰²ï¼Œé€‚åˆ N â‰« M
  - **2D å—åˆ†å‰²ï¼ˆBlock 2D / SUMMAï¼‰**ï¼šå°†è¿›ç¨‹æ’åˆ—ä¸º 2D ç½‘æ ¼ï¼ŒA æŒ‰è¡Œã€B æŒ‰åˆ—åŒæ—¶åˆ†å‰²ï¼Œå¤§å¹…é™ä½æ¯å¡æ˜¾å­˜
- æ”¯æŒå¼‚æ„é›†ç¾¤çš„è‡ªé€‚åº”è´Ÿè½½å‡è¡¡

### ğŸ¯ åˆ›æ–°ç‚¹2ï¼šè®¡ç®—-é€šä¿¡é‡å çš„æµæ°´çº¿ä¼˜åŒ–ç­–ç•¥
- åŸºäº CUDA åŒæµï¼ˆcompute\_stream / comm\_streamï¼‰å®ç° GPU è®¡ç®—ä¸ CPU ä¾§ MPI é€šä¿¡çš„çœŸæ­£é‡å 
- å°†çŸ©é˜µä¹˜æ³•åˆ†è§£ä¸º scatter â†’ compute â†’ gather ä¸‰é˜¶æ®µæµæ°´çº¿
- æ¨å¯¼æœ€ä¼˜åˆ†å—å¤§å°çš„è§£æè¡¨è¾¾å¼
- ç†è®ºä¸Šå½“è®¡ç®—å¯†é›†æ—¶å¯å®Œå…¨éšè—é€šä¿¡å»¶è¿Ÿ

### ğŸ¯ åˆ›æ–°ç‚¹3ï¼šé¢å‘ç§‘å­¦è®¡ç®—çš„åˆ›æ–°ç®—å­æ—

| åˆ›æ–°ç®—å­ | æ ¸å¿ƒæŠ€æœ¯ | åº”ç”¨åœºæ™¯ |
|---|---|---|
| **æ··åˆç²¾åº¦é€šä¿¡** | FP16 ä¼ è¾“ + FP32 è®¡ç®—ï¼Œé€šä¿¡é‡å‡åŠ | å¤§è§„æ¨¡çŸ©é˜µè¿ç®—çš„å¸¦å®½ä¼˜åŒ– |
| **Pencil åˆ†è§£ 2D FFT** | æ²¿å˜æ¢ç»´åº¦åˆ†å‰² + All-to-All è½¬ç½® | å•å¼ è¶…å¤§ç½‘æ ¼ï¼ˆç‰©ç†æ¨¡æ‹Ÿåœºæ•°æ®ï¼‰ |
| **Kahan è¡¥å¿æ±‚å’Œ** | float64 ä¸­é—´ç²¾åº¦ + è¡¥å¿ç®—æ³•ï¼Œè¯¯å·® O(Îµ) | èƒ½é‡å®ˆæ’éªŒè¯ã€é•¿æ—¶é—´ç§¯åˆ† |
| **ç¨€ç–æ„ŸçŸ¥è‡ªé€‚åº”** | è‡ªåŠ¨æ£€æµ‹ç¨€ç–åº¦ â†’ COO æ ¼å¼å¹¿æ’­ | æœ‰é™å…ƒåˆšåº¦çŸ©é˜µã€å›¾é‚»æ¥çŸ©é˜µ |
| **Stencil + Halo Exchange** | MPI Sendrecv æ— æ­»é”è¾¹ç•Œäº¤æ¢ + conv2d | çƒ­ä¼ å¯¼ã€æ³Šæ¾æ–¹ç¨‹ã€æ³¢åŠ¨æ–¹ç¨‹ |

### ğŸ¯ åˆ›æ–°ç‚¹4ï¼šé›†æˆ opt\_einsum çš„åˆ†å¸ƒå¼å¼ é‡æ”¶ç¼©
- è‡ªåŠ¨è®¡ç®—æœ€ä¼˜æ”¶ç¼©è·¯å¾„ï¼ˆæ”¯æŒ optimal / dp / greedy ç­‰ç­–ç•¥ï¼‰
- è·¯å¾„ä¼˜åŒ–ä¸åˆ†å¸ƒå¼å¹¶è¡ŒåŒé‡åŠ é€Ÿ
- æ”¯æŒä»»æ„ Einstein æ±‚å’Œè¡¨è¾¾å¼

### ğŸ¯ åˆ›æ–°ç‚¹5ï¼šæ˜¾å­˜æ„ŸçŸ¥çš„è‡ªé€‚åº”èµ„æºè°ƒåº¦
- **å®æ—¶æ‰«æ GPU å¯ç”¨æ˜¾å­˜**ï¼ˆä½¿ç”¨ `torch.cuda.mem_get_info()` è·å– OS çº§åˆ«çœŸå®å¯ç”¨æ˜¾å­˜ï¼Œæ­£ç¡®åæ˜ å…¶ä»–è¿›ç¨‹å ç”¨ï¼‰
- **æ™ºèƒ½æ‰§è¡Œè®¡åˆ’ç”Ÿæˆ**ï¼šæ ¹æ®æ•°æ®æ€»é‡å’Œå„å¡å¯ç”¨æ˜¾å­˜ï¼Œè‡ªåŠ¨å†³å®šåˆ†æ‰¹æ•°é‡å’Œåˆ†å‰²ç­–ç•¥
- **è¶…æ˜¾å­˜è‡ªåŠ¨åˆ†æ‰¹**ï¼ˆAuto-Batchingï¼‰ï¼šæ•°æ®é‡è¶…è¿‡ GPU æ€»æ˜¾å­˜æ—¶ï¼Œè‡ªåŠ¨æ²¿è¡Œç»´åº¦åˆ†æ‰¹å¤„ç†
  - MatMul ä¼˜åŒ–ï¼šB çŸ©é˜µåªå¹¿æ’­ä¸€æ¬¡ï¼Œåç»­æ‰¹æ¬¡å¤ç”¨
  - Conv2d ä¼˜åŒ–ï¼šweight/bias åªå¹¿æ’­ä¸€æ¬¡
- **20% å®‰å…¨è¾¹é™…**é˜²æ­¢ OOMï¼ˆå…¶ä»–è¿›ç¨‹ä¸´æ—¶åˆ†é…å¯¼è‡´çš„æ˜¾å­˜æ³¢åŠ¨ï¼‰
- **ä¸€è¡Œå¼ API**ï¼šç”¨æˆ·åªéœ€æä¾› CPU å¼ é‡ï¼Œæ¡†æ¶å…¨è‡ªåŠ¨å®Œæˆ GPU æ˜¾å­˜æ‰«æ â†’ èµ„æºè§„åˆ’ â†’ åˆ†æ‰¹æ‰§è¡Œ â†’ ç»“æœæ‹¼æ¥

### ğŸ›¡ï¸ é”™è¯¯å¤„ç†ä¸å®¹é”™
- è‡ªå®šä¹‰ `MPIError` å¼‚å¸¸ç±»ï¼Œé™„å¸¦ rank ä¿¡æ¯ä¾¿äºè°ƒè¯•
- `_safe_call` åŒ…è£…å™¨æ•è· MPI é€šä¿¡é”™è¯¯ï¼Œé¿å…æ— ä¿¡æ¯æ­»é”
- `check_health` è½»é‡çº§å¿ƒè·³æ£€æµ‹æ‰€æœ‰è¿›ç¨‹å­˜æ´»çŠ¶æ€
- scatter æ“ä½œå‰å¹¿æ’­é”™è¯¯ä¿¡æ¯ï¼Œç¡®ä¿æ‰€æœ‰ rank åŒæ­¥é€€å‡º

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/sxl19971024/distributed_gpu.git
cd distributed_gpu

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æˆ–ä½¿ç”¨ pip å®‰è£…
pip install -e .
```

### è¿è¡Œæµ‹è¯•

```bash
# ä½¿ç”¨4ä¸ªGPUè¿è¡Œç»¼åˆæµ‹è¯•ï¼ˆ17é¡¹ï¼‰
mpirun -n 4 python examples/run_algorithm.py all
```

é¢„æœŸè¾“å‡ºï¼š
```
æ€»è®¡: 17/17 é€šè¿‡
```

### åŸºæœ¬ä½¿ç”¨

```python
import torch
from distributed_gpu.mpi_manager import MPIManager
from distributed_gpu.tensor_distributor import TensorDistributor
from distributed_gpu.algorithms.matrix_ops import distributed_matmul

# åˆå§‹åŒ–ï¼ˆæ‰€æœ‰è¿›ç¨‹éƒ½å¿…é¡»æ‰§è¡Œï¼‰
mpi = MPIManager()
distributor = TensorDistributor(mpi)

# åˆ›å»ºæ•°æ®ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
if mpi.is_master_process():
    A = torch.randn(10000, 10000).cuda()
    B = torch.randn(10000, 10000).cuda()
else:
    A, B = None, None

# åˆ†å¸ƒå¼è®¡ç®—ï¼ˆæ‰€æœ‰è¿›ç¨‹éƒ½è°ƒç”¨ï¼‰
C = distributed_matmul(A, B, mpi, distributor)

# ç»“æœä»…åœ¨ä¸»è¿›ç¨‹
if mpi.is_master_process():
    print(f"ç»“æœå½¢çŠ¶: {C.shape}")
```

### ä½¿ç”¨ä»£ä»·æ¨¡å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥

```python
from distributed_gpu.cost_model import CostModel, ClusterConfig

config = ClusterConfig.from_auto_detect(num_nodes=4)
cost_model = CostModel(config)

# è‡ªåŠ¨é€‰æ‹©è¡Œåˆ†å‰² / åˆ—åˆ†å‰² / 2D å—åˆ†å‰²
C = distributed_matmul(A, B, mpi, distributor, cost_model=cost_model)
```

### ä½¿ç”¨åˆ›æ–°ç®—å­

```python
from distributed_gpu.algorithms.matrix_ops import distributed_matmul_mixed_precision
from distributed_gpu.algorithms.fft import distributed_fft2d_pencil
from distributed_gpu.algorithms.reduction import distributed_sum_kahan
from distributed_gpu.algorithms.stencil import distributed_stencil_2d

# æ··åˆç²¾åº¦çŸ©é˜µä¹˜æ³•ï¼ˆé€šä¿¡é‡å‡åŠï¼‰
C = distributed_matmul_mixed_precision(A, B, mpi, distributor)

# Pencil 2D FFTï¼ˆå¤„ç†å•å¼ è¶…å¤§ç½‘æ ¼ï¼‰
spectrum = distributed_fft2d_pencil(field, mpi, distributor)

# Kahan è¡¥å¿æ±‚å’Œï¼ˆæ•°å€¼ç¨³å®šï¼‰
total = distributed_sum_kahan(tensor, mpi, distributor)

# Stencil è®¡ç®—ï¼ˆç‰©ç†æ¨¡æ‹Ÿï¼‰
result = distributed_stencil_2d(grid, mpi, distributor, iterations=100)
```

### ä¸€è¡Œå¼è‡ªåŠ¨åŒ– APIï¼ˆæ˜¾å­˜æ„ŸçŸ¥ + è‡ªåŠ¨åˆ†æ‰¹ï¼‰

```python
from distributed_gpu.auto_executor import AutoExecutor

executor = AutoExecutor()  # åˆå§‹åŒ–ï¼ˆæ‰€æœ‰è¿›ç¨‹ï¼‰

# æŸ¥çœ‹ GPU å®æ—¶æ˜¾å­˜çŠ¶æ€
executor.gpu_status()

# ç”¨æˆ·åªéœ€æä¾› CPU å¼ é‡ï¼ˆä»… master è¿›ç¨‹ï¼‰
if executor.is_master:
    A = torch.randn(100000, 10000)  # 3.7 GBï¼Œå¯èƒ½è¶…å•å¡æ˜¾å­˜
    B = torch.randn(10000, 5000)
else:
    A = B = None

# æ¡†æ¶è‡ªåŠ¨ï¼šæ‰«ææ˜¾å­˜ â†’ ç”Ÿæˆåˆ†æ‰¹è®¡åˆ’ â†’ åˆ†å¸ƒå¼æ‰§è¡Œ â†’ è¿”å› CPU ç»“æœ
C = executor.matmul(A, B)  # è‡ªåŠ¨åˆ†æ‰¹ + è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åˆ†å‰²ç­–ç•¥

# åŒæ ·æ”¯æŒ FFTã€Sumã€Conv2dã€Einsum ç­‰
Y = executor.fft(signal)
S = executor.sum(data)
```

## ğŸ“š æ”¯æŒçš„ç®—å­ï¼ˆ25 ä¸ªï¼‰

| ç±»åˆ« | ç®—å­ | å‡½æ•° | è¯´æ˜ |
|------|------|------|------|
| **çŸ©é˜µè¿ç®—** | çŸ©é˜µä¹˜æ³• | `distributed_matmul` | è¡Œ/åˆ—/2Då—åˆ†å‰²ï¼Œä»£ä»·æ¨¡å‹è‡ªåŠ¨é€‰æ‹© |
| | æ‰¹é‡çŸ©é˜µä¹˜æ³• | `distributed_batch_matmul` | æŒ‰ batch ç»´åº¦å¹¶è¡Œ |
| | çŸ©é˜µè½¬ç½® | `distributed_transpose` | æ”¯æŒä»»æ„ç»´åº¦äº¤æ¢ |
| | å¼ é‡åŠ æ³• | `distributed_add` | é€å…ƒç´ å¹¶è¡Œ |
| | â­ æ··åˆç²¾åº¦çŸ©é˜µä¹˜æ³• | `distributed_matmul_mixed_precision` | FP16 é€šä¿¡ + FP32 è®¡ç®— |
| | â­ ç¨€ç–æ„ŸçŸ¥çŸ©é˜µä¹˜æ³• | `distributed_matmul_sparse_aware` | è‡ªåŠ¨æ£€æµ‹ç¨€ç–åº¦ï¼ŒCOO æ ¼å¼å¹¿æ’­ |
| **æµæ°´çº¿** | æµæ°´çº¿çŸ©é˜µä¹˜æ³• | `PipelineOptimizer.pipelined_matmul` | CUDA åŒæµè®¡ç®—-é€šä¿¡é‡å  |
| | æµæ°´çº¿ AllReduce | `PipelineOptimizer.pipelined_allreduce` | åˆ†å—å¼‚æ­¥å½’çº¦ |
| **å·ç§¯** | 2D å·ç§¯ | `distributed_conv2d` | æŒ‰ batch åˆ†å‰²ï¼Œæ”¯æŒ bias |
| **å‚…é‡Œå¶å˜æ¢** | 1D FFT | `distributed_fft` | æŒ‰ batch åˆ†å‰² |
| | 1D IFFT | `distributed_ifft` | é€†å˜æ¢ |
| | 2D FFT | `distributed_fft2d` | æŒ‰ batch åˆ†å‰² |
| | å®æ•° FFT | `distributed_rfft` | æ­£é¢‘ç‡ä¼˜åŒ–ï¼Œè®¡ç®—/é€šä¿¡é‡å‡åŠ |
| | â­ Pencil 2D FFT | `distributed_fft2d_pencil` | All-to-All è½¬ç½®ï¼Œæ”¯æŒè¶…å¤§å•ç½‘æ ¼ |
| **å¼ é‡æ”¶ç¼©** | Einstein æ±‚å’Œ | `distributed_einsum` | é›†æˆ opt\_einsum æœ€ä¼˜è·¯å¾„ |
| | å¸¦è·¯å¾„ Einstein æ±‚å’Œ | `distributed_einsum_with_path` | å¤ç”¨é¢„è®¡ç®—è·¯å¾„ |
| | å¼ é‡ç‚¹ç§¯ | `distributed_tensordot` | ä»»æ„ç»´åº¦æ”¶ç¼© |
| **å½’çº¦æ“ä½œ** | æ±‚å’Œ / å‡å€¼ | `distributed_sum` / `distributed_mean` | å…¨å±€æˆ–æŒ‰ç»´åº¦å½’çº¦ |
| | æœ€å¤§ / æœ€å°å€¼ | `distributed_max` / `distributed_min` | å…¨å±€æˆ–æŒ‰ç»´åº¦å½’çº¦ |
| | â­ Kahan è¡¥å¿æ±‚å’Œ | `distributed_sum_kahan` | è¯¯å·® O(Îµ) è€Œé O(nÂ·Îµ) |
| | â­ Kahan è¡¥å¿å‡å€¼ | `distributed_mean_kahan` | åŸºäº Kahan çš„é«˜ç²¾åº¦å‡å€¼ |
| **Stencil / PDE** | â­ 2D Stencil | `distributed_stencil_2d` | Halo Exchange + conv2d |
| | â­ Jacobi è¿­ä»£ | `distributed_jacobi_2d` | æ±‚è§£ âˆ‡Â²u = fï¼Œè‡ªåŠ¨æ”¶æ•›æ£€æµ‹ |

> â­ æ ‡è®°ä¸ºæœ¬æ¡†æ¶çš„**åˆ›æ–°ç®—å­**

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
distributed_gpu_framework/
â”œâ”€â”€ distributed_gpu/
â”‚   â”œâ”€â”€ __init__.py              # åŒ…å…¥å£ï¼Œå¯¼å‡ºæ ¸å¿ƒç±»
â”‚   â”œâ”€â”€ mpi_manager.py           # MPIé€šä¿¡ç®¡ç†å™¨ï¼ˆå«é”™è¯¯å¤„ç†/å®¹é”™ï¼‰
â”‚   â”œâ”€â”€ tensor_distributor.py    # å¼ é‡åˆ†é…å™¨ï¼ˆ1D/2Dåˆ†å‰² + æ··åˆç²¾åº¦å‹ç¼©ï¼‰
â”‚   â”œâ”€â”€ gpu_manager.py           # GPUè®¾å¤‡ç®¡ç† / æ˜¾å­˜ç›‘æ§
â”‚   â”œâ”€â”€ cost_model.py            # ä»£ä»·æ¨¡å‹ä¸è‡ªé€‚åº”ç­–ç•¥é€‰æ‹©
â”‚   â”œâ”€â”€ pipeline_optimizer.py    # CUDAåŒæµæµæ°´çº¿ä¼˜åŒ–
â”‚   â”œâ”€â”€ resource_planner.py     # æ˜¾å­˜æ„ŸçŸ¥èµ„æºè§„åˆ’å™¨ï¼ˆå®æ—¶æ‰«æ+åˆ†æ‰¹ç­–ç•¥ï¼‰
â”‚   â”œâ”€â”€ auto_executor.py        # è‡ªåŠ¨åŒ–æ‰§è¡Œå™¨ï¼ˆä¸€è¡Œå¼API+è¶…æ˜¾å­˜åˆ†æ‰¹ï¼‰
â”‚   â”œâ”€â”€ algorithms/              # åˆ†å¸ƒå¼ç®—æ³•åº“
â”‚   â”‚   â”œâ”€â”€ matrix_ops.py        # çŸ©é˜µè¿ç®—ï¼ˆè¡Œ/åˆ—/2D + æ··åˆç²¾åº¦ + ç¨€ç–æ„ŸçŸ¥ï¼‰
â”‚   â”‚   â”œâ”€â”€ convolution.py       # å·ç§¯æ“ä½œ
â”‚   â”‚   â”œâ”€â”€ fft.py               # FFTï¼ˆ1D/2D/å®æ•°/Pencilåˆ†è§£ï¼‰
â”‚   â”‚   â”œâ”€â”€ einsum.py            # Einsteinæ±‚å’Œï¼ˆé›†æˆopt_einsumï¼‰
â”‚   â”‚   â”œâ”€â”€ reduction.py         # å½’çº¦æ“ä½œï¼ˆå«Kahanè¡¥å¿æ±‚å’Œï¼‰
â”‚   â”‚   â””â”€â”€ stencil.py           # Stencilè®¡ç®— + Halo Exchange + Jacobiè¿­ä»£
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ profiler.py          # æ€§èƒ½åˆ†æå·¥å…·
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ run_algorithm.py         # ç®—æ³•æµ‹è¯•å·¥å…·ï¼ˆ24ä¸ªç®—å­ + äº¤äº’å¼é€‰æ‹©ï¼‰
â”‚   â””â”€â”€ test_auto_executor.py   # AutoExecutor æµ‹è¯•ï¼ˆè‡ªåŠ¨åˆ†æ‰¹/å•å¡å¿«é€Ÿè·¯å¾„ï¼‰
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ API_GUIDE.md             # API è¯¦ç»†ä½¿ç”¨æŒ‡å—
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ INSTALL.md                   # å®‰è£…æŒ‡å—
â””â”€â”€ README.md
```

## ğŸ’» ç³»ç»Ÿè¦æ±‚

- Python 3.8+
- CUDA 11.0+
- PyTorch 2.0+
- OpenMPI 4.0+ æˆ– MPICH
- å¤šä¸ª NVIDIA GPU

## ğŸ“– æ–‡æ¡£

- [å®‰è£…æŒ‡å—](INSTALL.md) â€” ç¯å¢ƒé…ç½®ä¸ä¾èµ–å®‰è£…
- [APIä½¿ç”¨æŒ‡å—](docs/API_GUIDE.md) â€” æ¯ä¸ªç®—æ³•çš„è¯¦ç»†å‚æ•°ã€ç¤ºä¾‹å’Œé«˜çº§åŠŸèƒ½

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ“§ è”ç³»æ–¹å¼

- ä½œè€…: å­™å°æ—
- Email: 1271364457@qq.com

## ğŸ“š å¼•ç”¨

å¦‚æœæœ¬æ¡†æ¶å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{distributed_gpu_framework,
  author = {å­™å°æ—},
  title = {MPI-based Distributed GPU Computing Framework for Scientific Computing},
  year = {2026},
  url = {https://github.com/sxl19971024/distributed_gpu}
}
```
